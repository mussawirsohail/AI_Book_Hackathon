"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[8175],{2853:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>u,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"modules/module-2-digital-twin/lesson-3-unity-integration","title":"Lesson 3: Unity Integration and High-Fidelity Rendering","description":"Introduction to Unity for Robotics","source":"@site/docs/modules/module-2-digital-twin/lesson-3-unity-integration.md","sourceDirName":"modules/module-2-digital-twin","slug":"/modules/module-2-digital-twin/lesson-3-unity-integration","permalink":"/AI_Book_Hackathon/docs/modules/module-2-digital-twin/lesson-3-unity-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/mussawirsohail/AI_Book_Hackathon/docs/modules/module-2-digital-twin/lesson-3-unity-integration.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3}}');var r=i(4848),o=i(8453);const a={sidebar_position:3},s="Lesson 3: Unity Integration and High-Fidelity Rendering",c={},l=[{value:"Introduction to Unity for Robotics",id:"introduction-to-unity-for-robotics",level:2},{value:"Unity ROS Integration",id:"unity-ros-integration",level:2},{value:"Setting up Unity with ROS 2",id:"setting-up-unity-with-ros-2",level:3},{value:"Basic ROS Connection in Unity",id:"basic-ros-connection-in-unity",level:3},{value:"High-Fidelity Rendering in Unity",id:"high-fidelity-rendering-in-unity",level:2},{value:"Physically Based Rendering (PBR)",id:"physically-based-rendering-pbr",level:3},{value:"Lighting Setup for Photorealism",id:"lighting-setup-for-photorealism",level:3},{value:"Human-Robot Interaction (HRI) in Unity",id:"human-robot-interaction-hri-in-unity",level:2},{value:"Creating Interactive Elements",id:"creating-interactive-elements",level:3},{value:"Voice Command Integration",id:"voice-command-integration",level:3},{value:"Unity vs Gazebo: When to Use Each",id:"unity-vs-gazebo-when-to-use-each",level:2},{value:"Use Gazebo when:",id:"use-gazebo-when",level:3},{value:"Use Unity when:",id:"use-unity-when",level:3},{value:"Unity Gazebo Bridge",id:"unity-gazebo-bridge",level:2},{value:"Creating Synthetic Training Data",id:"creating-synthetic-training-data",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,o.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"lesson-3-unity-integration-and-high-fidelity-rendering",children:"Lesson 3: Unity Integration and High-Fidelity Rendering"})}),"\n",(0,r.jsx)(e.h2,{id:"introduction-to-unity-for-robotics",children:"Introduction to Unity for Robotics"}),"\n",(0,r.jsx)(e.p,{children:"Unity is a powerful game engine that has been increasingly adopted in robotics for high-fidelity simulation, visualization, and human-robot interaction prototyping. Unlike Gazebo which is primarily focused on physics simulation, Unity excels at creating photorealistic environments and user interfaces."}),"\n",(0,r.jsx)(e.h2,{id:"unity-ros-integration",children:"Unity ROS Integration"}),"\n",(0,r.jsx)(e.p,{children:"Unity Robotics provides packages that enable communication with ROS 2, allowing you to use Unity as a visualization layer for your robotic systems or as a high-fidelity simulation environment."}),"\n",(0,r.jsx)(e.h3,{id:"setting-up-unity-with-ros-2",children:"Setting up Unity with ROS 2"}),"\n",(0,r.jsx)(e.p,{children:"To connect Unity to ROS 2, you can use the Unity Robotics Hub which includes:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"ROS TCP Connector: For communication between Unity and ROS 2"}),"\n",(0,r.jsx)(e.li,{children:"Unity Robotics Packages: For robotics-specific functionality"}),"\n",(0,r.jsx)(e.li,{children:"Sample projects and tutorials"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"basic-ros-connection-in-unity",children:"Basic ROS Connection in Unity"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\r\nusing Unity.Robotics.ROSTCPConnector;\r\nusing RosMessageTypes.Std;\r\n\r\npublic class UnityRosConnection : MonoBehaviour\r\n{\r\n    ROSConnection ros;\r\n    public string rosIPAddress = "127.0.0.1";\r\n    public int rosPort = 10000;\r\n\r\n    void Start()\r\n    {\r\n        ros = ROSConnection.instance;\r\n        ros.Initialize(rosIPAddress, rosPort);\r\n    }\r\n\r\n    void Update()\r\n    {\r\n        // Send data to ROS\r\n        StringMsg message = new StringMsg("Hello from Unity!");\r\n        ros.Send("unity_topic", message);\r\n    }\r\n\r\n    void OnMessageReceived(StringMsg msg)\r\n    {\r\n        Debug.Log("Received from ROS: " + msg.data);\r\n    }\r\n}\n'})}),"\n",(0,r.jsx)(e.h2,{id:"high-fidelity-rendering-in-unity",children:"High-Fidelity Rendering in Unity"}),"\n",(0,r.jsx)(e.p,{children:"Unity's rendering capabilities enable the creation of photorealistic environments that can be used for synthetic data generation and testing perception algorithms."}),"\n",(0,r.jsx)(e.h3,{id:"physically-based-rendering-pbr",children:"Physically Based Rendering (PBR)"}),"\n",(0,r.jsx)(e.p,{children:"Unity's PBR system accurately simulates how light interacts with surfaces:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'// Adjust material properties for realistic rendering\r\nvoid SetRealisticMaterial(GameObject obj, float metalness, float smoothness)\r\n{\r\n    Material material = obj.GetComponent<Renderer>().material;\r\n    material.SetFloat("_Metallic", metalness);\r\n    material.SetFloat("_Smoothness", smoothness);\r\n}\n'})}),"\n",(0,r.jsx)(e.h3,{id:"lighting-setup-for-photorealism",children:"Lighting Setup for Photorealism"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:"public class LightingSetup : MonoBehaviour\r\n{\r\n    public Light sunLight;\r\n    public ReflectionProbe reflectionProbe;\r\n\r\n    void Start()\r\n    {\r\n        // Configure directional light to simulate sun\r\n        sunLight.type = LightType.Directional;\r\n        sunLight.intensity = 1.0f;\r\n        sunLight.color = Color.white;\r\n        sunLight.transform.rotation = Quaternion.Euler(50, -30, 0);\r\n\r\n        // Update reflection probe for realistic reflections\r\n        reflectionProbe.RenderProbe();\r\n    }\r\n}\n"})}),"\n",(0,r.jsx)(e.h2,{id:"human-robot-interaction-hri-in-unity",children:"Human-Robot Interaction (HRI) in Unity"}),"\n",(0,r.jsx)(e.p,{children:"Unity is ideal for prototyping HRI interfaces, allowing you to create interactive environments where humans can interact with virtual robots."}),"\n",(0,r.jsx)(e.h3,{id:"creating-interactive-elements",children:"Creating Interactive Elements"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\r\nusing UnityEngine.UI;\r\n\r\npublic class HriInterface : MonoBehaviour\r\n{\r\n    public Button moveForwardButton;\r\n    public Button turnLeftButton;\r\n    public Button turnRightButton;\r\n\r\n    void Start()\r\n    {\r\n        moveForwardButton.onClick.AddListener(() => SendCommand("move_forward"));\r\n        turnLeftButton.onClick.AddListener(() => SendCommand("turn_left"));\r\n        turnRightButton.onClick.AddListener(() => SendCommand("turn_right"));\r\n    }\r\n\r\n    void SendCommand(string command)\r\n    {\r\n        // Send command to ROS via TCP connector\r\n        ROSConnection ros = ROSConnection.instance;\r\n        ros.Send("robot_command", new StringMsg(command));\r\n    }\r\n}\n'})}),"\n",(0,r.jsx)(e.h3,{id:"voice-command-integration",children:"Voice Command Integration"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\r\nusing System.Collections;\r\n\r\npublic class VoiceCommand : MonoBehaviour\r\n{\r\n    // This would integrate with a speech recognition system\r\n    // or connect to a ROS voice recognition node\r\n\r\n    void ProcessVoiceCommand(string command)\r\n    {\r\n        switch (command.ToLower())\r\n        {\r\n            case "move forward":\r\n                SendCommandToRobot("move_forward");\r\n                break;\r\n            case "turn left":\r\n                SendCommandToRobot("turn_left");\r\n                break;\r\n            case "stop":\r\n                SendCommandToRobot("stop");\r\n                break;\r\n        }\r\n    }\r\n\r\n    void SendCommandToRobot(string command)\r\n    {\r\n        ROSConnection ros = ROSConnection.instance;\r\n        ros.Send("voice_command", new StringMsg(command));\r\n    }\r\n}\n'})}),"\n",(0,r.jsx)(e.h2,{id:"unity-vs-gazebo-when-to-use-each",children:"Unity vs Gazebo: When to Use Each"}),"\n",(0,r.jsx)(e.h3,{id:"use-gazebo-when",children:"Use Gazebo when:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Accurate physics simulation is critical"}),"\n",(0,r.jsx)(e.li,{children:"Testing control algorithms"}),"\n",(0,r.jsx)(e.li,{children:"Working with standard robot models"}),"\n",(0,r.jsx)(e.li,{children:"Performance is a concern for large-scale simulations"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"use-unity-when",children:"Use Unity when:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"High-quality visual rendering is needed"}),"\n",(0,r.jsx)(e.li,{children:"Creating user interfaces and HRI prototypes"}),"\n",(0,r.jsx)(e.li,{children:"Generating synthetic training data for machine learning"}),"\n",(0,r.jsx)(e.li,{children:"Developing augmented reality interfaces"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"unity-gazebo-bridge",children:"Unity Gazebo Bridge"}),"\n",(0,r.jsx)(e.p,{children:"For the best of both worlds, you can connect Unity and Gazebo:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Use Gazebo for physics simulation"}),"\n",(0,r.jsx)(e.li,{children:"Use Unity for visualization"}),"\n",(0,r.jsx)(e.li,{children:"Synchronize state between both environments"}),"\n"]}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'// Example of synchronizing robot state between Gazebo and Unity\r\npublic class RobotStateSynchronizer : MonoBehaviour\r\n{\r\n    public GameObject robotModel;\r\n    ROSConnection ros;\r\n    \r\n    void Start()\r\n    {\r\n        ros = ROSConnection.instance;\r\n        ros.Subscribe<OdometryMsg>("/robot/odom", UpdateRobotPosition);\r\n    }\r\n\r\n    void UpdateRobotPosition(OdometryMsg odom)\r\n    {\r\n        // Update Unity robot position based on Gazebo simulation\r\n        Vector3 position = new Vector3(\r\n            (float)odom.pose.pose.position.x,\r\n            (float)odom.pose.pose.position.y,\r\n            (float)odom.pose.pose.position.z\r\n        );\r\n        \r\n        Quaternion rotation = new Quaternion(\r\n            (float)odom.pose.pose.orientation.x,\r\n            (float)odom.pose.pose.orientation.y,\r\n            (float)odom.pose.pose.orientation.z,\r\n            (float)odom.pose.pose.orientation.w\r\n        );\r\n        \r\n        robotModel.transform.position = position;\r\n        robotModel.transform.rotation = rotation;\r\n    }\r\n}\n'})}),"\n",(0,r.jsx)(e.h2,{id:"creating-synthetic-training-data",children:"Creating Synthetic Training Data"}),"\n",(0,r.jsx)(e.p,{children:"Unity's rendering capabilities make it ideal for generating synthetic data for training machine learning models:"}),"\n",(0,r.jsx)(e.pre,{children:(0,r.jsx)(e.code,{className:"language-csharp",children:'using UnityEngine;\r\n\r\npublic class SyntheticDataGenerator : MonoBehaviour\r\n{\r\n    public Camera sensorCamera;\r\n    public int datasetSize = 1000;\r\n    private int currentSample = 0;\r\n\r\n    void Start()\r\n    {\r\n        StartCoroutine(GenerateDataset());\r\n    }\r\n\r\n    IEnumerator GenerateDataset()\r\n    {\r\n        while (currentSample < datasetSize)\r\n        {\r\n            // Move objects to random positions\r\n            MoveSceneObjectsRandomly();\r\n            \r\n            // Capture image from sensor camera\r\n            yield return new WaitForEndOfFrame();\r\n            CaptureImageAndDepth(currentSample);\r\n            \r\n            currentSample++;\r\n            yield return new WaitForSeconds(0.5f); // Wait between captures\r\n        }\r\n    }\r\n\r\n    void CaptureImageAndDepth(int index)\r\n    {\r\n        // Capture RGB image\r\n        Texture2D rgbImage = CaptureCameraImage(sensorCamera);\r\n        SaveImage(rgbImage, $"rgb_{index}.png");\r\n\r\n        // Capture depth image using depth shader\r\n        Texture2D depthImage = CaptureDepthImage(sensorCamera);\r\n        SaveImage(depthImage, $"depth_{index}.png");\r\n    }\r\n\r\n    Texture2D CaptureCameraImage(Camera cam)\r\n    {\r\n        RenderTexture currentRT = RenderTexture.active;\r\n        RenderTexture.active = cam.targetTexture;\r\n\r\n        cam.Render();\r\n        Texture2D image = new Texture2D(cam.targetTexture.width, cam.targetTexture.height);\r\n        image.ReadPixels(new Rect(0, 0, cam.targetTexture.width, cam.targetTexture.height), 0, 0);\r\n\r\n        RenderTexture.active = currentRT;\r\n        return image;\r\n    }\r\n}\n'})}),"\n",(0,r.jsx)(e.p,{children:"In the next module, we'll explore the AI-Robot Brain using NVIDIA Isaac, which provides advanced perception and training capabilities that can be enhanced with the synthetic data generation techniques we've discussed."})]})}function u(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>s});var t=i(6540);const r={},o=t.createContext(r);function a(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:a(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);
"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[416],{5483:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>t,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"modules/module-2-digital-twin/lesson-2-sensor-simulation","title":"Lesson 2: Simulating Sensors in Gazebo and Unity","description":"Sensor Simulation Overview","source":"@site/docs/modules/module-2-digital-twin/lesson-2-sensor-simulation.md","sourceDirName":"modules/module-2-digital-twin","slug":"/modules/module-2-digital-twin/lesson-2-sensor-simulation","permalink":"/AI_Book_Hackathon/docs/modules/module-2-digital-twin/lesson-2-sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/mussawirsohail/AI_Book_Hackathon/edit/main/docs/modules/module-2-digital-twin/lesson-2-sensor-simulation.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 1: Introduction to Gazebo Simulation","permalink":"/AI_Book_Hackathon/docs/modules/module-2-digital-twin/lesson-1-gazebo-simulation"},"next":{"title":"Lesson 3: Unity Integration and High-Fidelity Rendering","permalink":"/AI_Book_Hackathon/docs/modules/module-2-digital-twin/lesson-3-unity-integration"}}');var a=r(4848),s=r(8453);const o={sidebar_position:2},t="Lesson 2: Simulating Sensors in Gazebo and Unity",l={},c=[{value:"Sensor Simulation Overview",id:"sensor-simulation-overview",level:2},{value:"LiDAR Simulation in Gazebo",id:"lidar-simulation-in-gazebo",level:2},{value:"LiDAR Configuration in SDF",id:"lidar-configuration-in-sdf",level:3},{value:"Accessing LiDAR Data in ROS 2",id:"accessing-lidar-data-in-ros-2",level:3},{value:"Depth Camera Simulation",id:"depth-camera-simulation",level:2},{value:"Depth Camera Configuration in SDF",id:"depth-camera-configuration-in-sdf",level:3},{value:"Processing Depth Camera Data",id:"processing-depth-camera-data",level:3},{value:"IMU Simulation",id:"imu-simulation",level:2},{value:"IMU Configuration in SDF",id:"imu-configuration-in-sdf",level:3},{value:"IMU Data Processing",id:"imu-data-processing",level:3},{value:"Unity Sensor Simulation",id:"unity-sensor-simulation",level:2},{value:"Unity LiDAR Simulation",id:"unity-lidar-simulation",level:3},{value:"Sensor Fusion in Simulation",id:"sensor-fusion-in-simulation",level:2}];function m(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",pre:"pre",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"lesson-2-simulating-sensors-in-gazebo-and-unity",children:"Lesson 2: Simulating Sensors in Gazebo and Unity"})}),"\n",(0,a.jsx)(e.h2,{id:"sensor-simulation-overview",children:"Sensor Simulation Overview"}),"\n",(0,a.jsx)(e.p,{children:"Accurate sensor simulation is crucial for developing and testing robotic systems. In digital twin environments like Gazebo and Unity, we can simulate various sensors with realistic noise models and environmental effects."}),"\n",(0,a.jsx)(e.h2,{id:"lidar-simulation-in-gazebo",children:"LiDAR Simulation in Gazebo"}),"\n",(0,a.jsx)(e.p,{children:"LiDAR (Light Detection and Ranging) sensors are essential for navigation and mapping. Simulating LiDAR in Gazebo involves configuring ray sensors that mimic real LiDAR behavior."}),"\n",(0,a.jsx)(e.h3,{id:"lidar-configuration-in-sdf",children:"LiDAR Configuration in SDF"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<sensor name="lidar_sensor" type="ray">\r\n  <pose>0 0 0.2 0 0 0</pose>\r\n  <visualize>true</visualize>\r\n  <update_rate>10</update_rate>\r\n  <ray>\r\n    <scan>\r\n      <horizontal>\r\n        <samples>720</samples>\r\n        <resolution>1</resolution>\r\n        <min_angle>-3.14159</min_angle>\r\n        <max_angle>3.14159</max_angle>\r\n      </horizontal>\r\n    </scan>\r\n    <range>\r\n      <min>0.1</min>\r\n      <max>30.0</max>\r\n      <resolution>0.01</resolution>\r\n    </range>\r\n  </ray>\r\n  <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\r\n    <ros>\r\n      <namespace>/lidar</namespace>\r\n      <remapping>~/out:=scan</remapping>\r\n    </ros>\r\n    <output_type>sensor_msgs/LaserScan</output_type>\r\n    <frame_name>lidar_frame</frame_name>\r\n  </plugin>\r\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"accessing-lidar-data-in-ros-2",children:"Accessing LiDAR Data in ROS 2"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan\r\n\r\nclass LidarSubscriber(Node):\r\n    def __init__(self):\r\n        super().__init__('lidar_subscriber')\r\n        self.subscription = self.create_subscription(\r\n            LaserScan,\r\n            '/scan',\r\n            self.lidar_callback,\r\n            10\r\n        )\r\n\r\n    def lidar_callback(self, msg):\r\n        # Process LiDAR data\r\n        ranges = msg.ranges\r\n        min_distance = min(ranges)\r\n        self.get_logger().info(f'Min distance: {min_distance}')\n"})}),"\n",(0,a.jsx)(e.h2,{id:"depth-camera-simulation",children:"Depth Camera Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Depth cameras provide both color and depth information, which is valuable for 3D environment mapping and object recognition."}),"\n",(0,a.jsx)(e.h3,{id:"depth-camera-configuration-in-sdf",children:"Depth Camera Configuration in SDF"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<sensor name="depth_camera" type="depth">\r\n  <pose>0.1 0 0.1 0 0 0</pose>\r\n  <visualize>true</visualize>\r\n  <update_rate>30</update_rate>\r\n  <camera name="head">\r\n    <horizontal_fov>1.047</horizontal_fov>\r\n    <image>\r\n      <width>640</width>\r\n      <height>480</height>\r\n      <format>R8G8B8</format>\r\n    </image>\r\n    <clip>\r\n      <near>0.1</near>\r\n      <far>100</far>\r\n    </clip>\r\n    <noise>\r\n      <type>gaussian</type>\r\n      <mean>0.0</mean>\r\n      <stddev>0.007</stddev>\r\n    </noise>\r\n  </camera>\r\n  <plugin name="camera_controller" filename="libgazebo_ros_openni_kinect.so">\r\n    <baseline>0.2</baseline>\r\n    <always_on>true</always_on>\r\n    <update_rate>30.0</update_rate>\r\n    <camera_name>camera</camera_name>\r\n    <image_topic_name>rgb/image_raw</image_topic_name>\r\n    <depth_image_topic_name>depth/image_raw</depth_image_topic_name>\r\n    <point_cloud_topic_name>depth/points</point_cloud_topic_name>\r\n    <camera_info_topic_name>rgb/camera_info</camera_info_topic_name>\r\n    <depth_image_camera_info_topic_name>depth/camera_info</depth_image_camera_info_topic_name>\r\n    <frame_name>camera_depth_optical_frame</frame_name>\r\n    <point_cloud_cutoff>0.5</point_cloud_cutoff>\r\n    <point_cloud_cutoff_max>3.0</point_cloud_cutoff_max>\r\n    <Cx>0</Cx>\r\n    <Cy>0</Cy>\r\n    <hack_baseline>0</hack_baseline>\r\n  </plugin>\r\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"processing-depth-camera-data",children:"Processing Depth Camera Data"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\nimport numpy as np\r\n\r\nclass DepthCameraSubscriber(Node):\r\n    def __init__(self):\r\n        super().__init__('depth_camera_subscriber')\r\n        self.bridge = CvBridge()\r\n        \r\n        # Subscribe to depth image\r\n        self.depth_subscription = self.create_subscription(\r\n            Image,\r\n            '/camera/depth/image_raw',\r\n            self.depth_image_callback,\r\n            10\r\n        )\r\n\r\n    def depth_image_callback(self, msg):\r\n        # Convert ROS Image message to OpenCV format\r\n        cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')\r\n        \r\n        # Process depth information\r\n        distance_at_center = cv_image[int(cv_image.shape[0]/2), int(cv_image.shape[1]/2)]\r\n        self.get_logger().info(f'Distance at center pixel: {distance_at_center} meters')\n"})}),"\n",(0,a.jsx)(e.h2,{id:"imu-simulation",children:"IMU Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Inertial Measurement Units (IMUs) provide information about acceleration, angular velocity, and orientation."}),"\n",(0,a.jsx)(e.h3,{id:"imu-configuration-in-sdf",children:"IMU Configuration in SDF"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-xml",children:'<sensor name="imu_sensor" type="imu">\r\n  <always_on>true</always_on>\r\n  <update_rate>100</update_rate>\r\n  <visualize>false</visualize>\r\n  <topic>__default_topic__</topic>\r\n  <plugin name="imu_plugin" filename="libgazebo_ros_imu.so">\r\n    <ros>\r\n      <namespace>/imu</namespace>\r\n      <remapping>~/out:=imu/data</remapping>\r\n    </ros>\r\n    <sensor>imu_sensor</sensor>\r\n    <gaussian_noise>0.00057</gaussian_noise>\r\n    <frame_name>imu_link</frame_name>\r\n  </plugin>\r\n</sensor>\n'})}),"\n",(0,a.jsx)(e.h3,{id:"imu-data-processing",children:"IMU Data Processing"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Imu\r\nimport math\r\n\r\nclass ImuSubscriber(Node):\r\n    def __init__(self):\r\n        super().__init__('imu_subscriber')\r\n        self.subscription = self.create_subscription(\r\n            Imu,\r\n            '/imu/data',\r\n            self.imu_callback,\r\n            10\r\n        )\r\n\r\n    def imu_callback(self, msg):\r\n        # Extract orientation quaternion\r\n        orientation_q = msg.orientation\r\n        # Convert to Euler angles\r\n        (roll, pitch, yaw) = self.euler_from_quaternion(\r\n            [orientation_q.x, orientation_q.y, orientation_q.z, orientation_q.w]\r\n        )\r\n        \r\n        self.get_logger().info(f'Roll: {roll}, Pitch: {pitch}, Yaw: {yaw}')\r\n\r\n    def euler_from_quaternion(self, quaternion):\r\n        x = quaternion[0]\r\n        y = quaternion[1]\r\n        z = quaternion[2]\r\n        w = quaternion[3]\r\n\r\n        sinr_cosp = 2 * (w * x + y * z)\r\n        cosr_cosp = 1 - 2 * (x * x + y * y)\r\n        roll = math.atan2(sinr_cosp, cosr_cosp)\r\n\r\n        sinp = 2 * (w * y - z * x)\r\n        pitch = math.asin(sinp)\r\n\r\n        siny_cosp = 2 * (w * z + x * y)\r\n        cosy_cosp = 1 - 2 * (y * y + z * z)\r\n        yaw = math.atan2(siny_cosp, cosy_cosp)\r\n\r\n        return roll, pitch, yaw\n"})}),"\n",(0,a.jsx)(e.h2,{id:"unity-sensor-simulation",children:"Unity Sensor Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Unity provides high-fidelity rendering and can simulate sensors through custom scripts and plugins. Unity Robotics provides packages for sensor simulation and integration with ROS."}),"\n",(0,a.jsx)(e.h3,{id:"unity-lidar-simulation",children:"Unity LiDAR Simulation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-csharp",children:"using UnityEngine;\r\nusing System.Collections.Generic;\r\n\r\npublic class UnityLidar : MonoBehaviour\r\n{\r\n    public int rays = 360;\r\n    public float range = 10f;\r\n    public float angle = 360f;\r\n    public LayerMask layerMask;\r\n\r\n    void Update()\r\n    {\r\n        var hits = new List<float>();\r\n        float angleStep = angle / rays;\r\n\r\n        for (int i = 0; i < rays; i++)\r\n        {\r\n            float currentAngle = transform.eulerAngles.y + i * angleStep;\r\n            Vector3 direction = Quaternion.Euler(0, currentAngle, 0) * transform.forward;\r\n\r\n            if (Physics.Raycast(transform.position, direction, out RaycastHit hit, range, layerMask))\r\n            {\r\n                hits.Add(hit.distance);\r\n            }\r\n            else\r\n            {\r\n                hits.Add(range);\r\n            }\r\n        }\r\n\r\n        // Process LiDAR data (send via ROS or use internally)\r\n        ProcessLidarData(hits);\r\n    }\r\n\r\n    void ProcessLidarData(List<float> distances)\r\n    {\r\n        // Send distances to ROS bridge or use in Unity\r\n    }\r\n}\n"})}),"\n",(0,a.jsx)(e.h2,{id:"sensor-fusion-in-simulation",children:"Sensor Fusion in Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Combining data from multiple sensors improves perception accuracy:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import LaserScan, Imu, Image\r\nfrom geometry_msgs.msg import Pose\r\nfrom tf2_ros import TransformBroadcaster\r\nimport numpy as np\r\n\r\nclass SensorFusionNode(Node):\r\n    def __init__(self):\r\n        super().__init__('sensor_fusion_node')\r\n        \r\n        # Initialize sensor data storage\r\n        self.lidar_data = None\r\n        self.imu_data = None\r\n        self.camera_data = None\r\n        \r\n        # Create subscribers for each sensor\r\n        self.lidar_sub = self.create_subscription(\r\n            LaserScan, '/scan', self.lidar_callback, 10)\r\n        self.imu_sub = self.create_subscription(\r\n            Imu, '/imu/data', self.imu_callback, 10)\r\n        self.camera_sub = self.create_subscription(\r\n            Image, '/camera/rgb/image_raw', self.camera_callback, 10)\r\n\r\n    def lidar_callback(self, msg):\r\n        self.lidar_data = msg.ranges\r\n\r\n    def imu_callback(self, msg):\r\n        self.imu_data = msg.orientation\r\n\r\n    def camera_callback(self, msg):\r\n        # Process camera data and combine with other sensors\r\n        pass\r\n\r\n    def sensor_fusion_algorithm(self):\r\n        # Algorithm to combine sensor data for state estimation\r\n        # This is a simplified example\r\n        if self.lidar_data and self.imu_data:\r\n            # Example: combine LiDAR obstacle detection with IMU orientation\r\n            # to improve navigation decisions\r\n            pass\n"})}),"\n",(0,a.jsxs)(e.p,{children:["Continue to ",(0,a.jsx)(e.a,{href:"/AI_Book_Hackathon/docs/modules/module-2-digital-twin/lesson-3-unity-integration",children:"Lesson 3: Unity Integration and High-Fidelity Rendering"})," to explore advanced rendering and human-robot interaction in Unity."]})]})}function d(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(m,{...n})}):m(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>o,x:()=>t});var i=r(6540);const a={},s=i.createContext(a);function o(n){const e=i.useContext(s);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function t(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:o(n.components),i.createElement(s.Provider,{value:e},n.children)}}}]);
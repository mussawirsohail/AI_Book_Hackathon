"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[1613],{362:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>d,frontMatter:()=>a,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"modules/module-4-vla/lesson-3-cognitive-planning","title":"Lesson 3: Cognitive Planning with LLMs and Capstone Project: The Autonomous Humanoid","description":"Cognitive Planning Overview","source":"@site/docs/modules/module-4-vla/lesson-3-cognitive-planning.md","sourceDirName":"modules/module-4-vla","slug":"/modules/module-4-vla/lesson-3-cognitive-planning","permalink":"/AI_Book_Hackathon/docs/modules/module-4-vla/lesson-3-cognitive-planning","draft":false,"unlisted":false,"editUrl":"https://github.com/mussawirsohail/AI_Book_Hackathon/edit/main/docs/modules/module-4-vla/lesson-3-cognitive-planning.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 2: Voice-to-Action with OpenAI Whisper for Robotic Control","permalink":"/AI_Book_Hackathon/docs/modules/module-4-vla/lesson-2-voice-to-action"},"next":{"title":"Lesson 1: Introduction to RAG Chatbot Development","permalink":"/AI_Book_Hackathon/docs/modules/module-5-rag-chatbot/lesson-1-introduction"}}');var t=r(4848),i=r(8453);const a={sidebar_position:3},s="Lesson 3: Cognitive Planning with LLMs and Capstone Project: The Autonomous Humanoid",c={},l=[{value:"Cognitive Planning Overview",id:"cognitive-planning-overview",level:2},{value:"LLM-Based Task Planning",id:"llm-based-task-planning",level:2},{value:"Basic Task Planning with LLMs",id:"basic-task-planning-with-llms",level:3},{value:"Capstone Project: The Autonomous Humanoid",id:"capstone-project-the-autonomous-humanoid",level:2},{value:"System Architecture",id:"system-architecture",level:3},{value:"Complete Autonomous Humanoid Node",id:"complete-autonomous-humanoid-node",level:3},{value:"System Integration and Testing",id:"system-integration-and-testing",level:2},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Evaluation Metrics",id:"evaluation-metrics",level:2},{value:"Conclusion",id:"conclusion",level:2}];function m(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"lesson-3-cognitive-planning-with-llms-and-capstone-project-the-autonomous-humanoid",children:"Lesson 3: Cognitive Planning with LLMs and Capstone Project: The Autonomous Humanoid"})}),"\n",(0,t.jsx)(e.h2,{id:"cognitive-planning-overview",children:"Cognitive Planning Overview"}),"\n",(0,t.jsx)(e.p,{children:"Cognitive planning in robotics involves translating high-level natural language commands into executable robotic actions. This requires reasoning about the environment, understanding object affordances, and decomposing complex tasks into primitive actions."}),"\n",(0,t.jsx)(e.h2,{id:"llm-based-task-planning",children:"LLM-Based Task Planning"}),"\n",(0,t.jsx)(e.p,{children:"Large Language Models (LLMs) excel at understanding natural language and can be used to decompose high-level commands into sequences of robotic actions."}),"\n",(0,t.jsx)(e.h3,{id:"basic-task-planning-with-llms",children:"Basic Task Planning with LLMs"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom moveit_msgs.msg import MoveItCommand\r\nfrom sensor_msgs.msg import Image\r\nfrom cv_bridge import CvBridge\r\nimport json\r\n\r\nclass CognitivePlanner(Node):\r\n    def __init__(self):\r\n        super().__init__(\'cognitive_planner\')\r\n        \r\n        # Initialize components\r\n        self.bridge = CvBridge()\r\n        \r\n        # Subscriptions\r\n        self.command_sub = self.create_subscription(\r\n            String,\r\n            \'/natural_language_command\',\r\n            self.command_callback,\r\n            10\r\n        )\r\n        \r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/rgb/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n        \r\n        # Publishers\r\n        self.action_pub = self.create_publisher(String, \'/robot_action\', 10)\r\n        self.navigation_pub = self.create_publisher(PoseStamped, \'/navigation_goal\', 10)\r\n        \r\n        # State\r\n        self.current_image = None\r\n        self.llm_client = self.initialize_llm()  # Initialize your preferred LLM\r\n        \r\n    def initialize_llm(self):\r\n        # Initialize your LLM client (OpenAI, Anthropic, local model, etc.)\r\n        # For this example, we\'ll create a mock implementation\r\n        class MockLLM:\r\n            def generate_response(self, prompt):\r\n                # This would be replaced with actual LLM call\r\n                return self.mock_plan_response(prompt)\r\n                \r\n            def mock_plan_response(self, prompt):\r\n                # Mock response for demonstration\r\n                return json.dumps({\r\n                    "task_decomposition": [\r\n                        {"action": "scan_environment", "description": "Scan the room to identify objects"},\r\n                        {"action": "identify_target", "description": "Locate the specific object to interact with"},\r\n                        {"action": "plan_path", "description": "Calculate path to the object"},\r\n                        {"action": "navigate", "description": "Move to the object location"},\r\n                        {"action": "manipulate", "description": "Perform the required manipulation action"},\r\n                        {"action": "return", "description": "Return to default position"}\r\n                    ],\r\n                    "object_location": [1.5, 2.0, 0.0],\r\n                    "action_sequence": ["scan", "identify", "navigate", "manipulate"]\r\n                })\r\n        \r\n        return MockLLM()\r\n        \r\n    def command_callback(self, msg):\r\n        command = msg.data\r\n        self.get_logger().info(f"Received command: {command}")\r\n        \r\n        if self.current_image is not None:\r\n            self.plan_and_execute(command, self.current_image)\r\n        else:\r\n            self.get_logger().warn("No image available, waiting...")\r\n            \r\n    def image_callback(self, msg):\r\n        self.current_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\r\n        \r\n    def plan_and_execute(self, command, image):\r\n        """Plan and execute the command using LLM reasoning"""\r\n        # Create a prompt for the LLM\r\n        prompt = self.create_planning_prompt(command, image)\r\n        \r\n        try:\r\n            # Get plan from LLM\r\n            plan_json = self.llm_client.generate_response(prompt)\r\n            plan = json.loads(plan_json)\r\n            \r\n            # Execute the plan\r\n            self.execute_plan(plan, command)\r\n            \r\n        except Exception as e:\r\n            self.get_logger().error(f"Error in planning or execution: {e}")\r\n            \r\n    def create_planning_prompt(self, command, image):\r\n        """Create a prompt for the LLM with context"""\r\n        prompt = f"""\r\n        You are a cognitive planning system for a humanoid robot. Your task is to decompose a \r\n        natural language command into a sequence of executable robotic actions.\r\n        \r\n        Command: "{command}"\r\n        \r\n        Current environment: [The robot has captured an image of its environment]\r\n        \r\n        Please provide:\r\n        1. Task decomposition: Break down the command into logical steps\r\n        2. Object identification: Identify the relevant objects in the environment\r\n        3. Action sequence: The sequence of actions to execute\r\n        4. Spatial information: Locations where actions should occur\r\n        \r\n        Respond in JSON format with the following structure:\r\n        {{\r\n            "task_decomposition": [\r\n                {{"action": "action_type", "description": "what to do"}},\r\n                ...\r\n            ],\r\n            "object_location": [x, y, z],\r\n            "action_sequence": ["action1", "action2", ...]\r\n        }}\r\n        \r\n        The robot has the following capabilities:\r\n        - Navigation: Move to specific coordinates\r\n        - Manipulation: Grasp and move objects\r\n        - Perception: Detect and recognize objects\r\n        - Interaction: Communicate with humans\r\n        """\r\n        \r\n        return prompt\r\n        \r\n    def execute_plan(self, plan, original_command):\r\n        """Execute the plan provided by the LLM"""\r\n        self.get_logger().info(f"Executing plan for command: {original_command}")\r\n        \r\n        for step in plan["task_decomposition"]:\r\n            action_type = step["action"]\r\n            description = step["description"]\r\n            \r\n            self.get_logger().info(f"Executing: {action_type} - {description}")\r\n            \r\n            if action_type == "scan_environment":\r\n                self.execute_scan()\r\n            elif action_type == "identify_target":\r\n                self.execute_identify_target()\r\n            elif action_type == "plan_path":\r\n                self.execute_plan_path()\r\n            elif action_type == "navigate":\r\n                self.execute_navigation(plan.get("object_location", [0, 0, 0]))\r\n            elif action_type == "manipulate":\r\n                self.execute_manipulation()\r\n            elif action_type == "return":\r\n                self.execute_return_home()\r\n            else:\r\n                self.get_logger().warn(f"Unknown action type: {action_type}")\r\n    \r\n    def execute_scan(self):\r\n        """Execute environment scanning"""\r\n        # Publish a command to perception system to scan environment\r\n        cmd_msg = String()\r\n        cmd_msg.data = "scan_environment"\r\n        self.action_pub.publish(cmd_msg)\r\n        \r\n    def execute_identify_target(self):\r\n        """Execute target object identification"""\r\n        cmd_msg = String()\r\n        cmd_msg.data = "identify_target_object"\r\n        self.action_pub.publish(cmd_msg)\r\n        \r\n    def execute_plan_path(self):\r\n        """Execute path planning"""\r\n        cmd_msg = String()\r\n        cmd_msg.data = "plan_path_to_target"\r\n        self.action_pub.publish(cmd_msg)\r\n        \r\n    def execute_navigation(self, target_location):\r\n        """Execute navigation to target location"""\r\n        goal = PoseStamped()\r\n        goal.header.stamp = self.get_clock().now().to_msg()\r\n        goal.header.frame_id = "map"\r\n        goal.pose.position.x = target_location[0]\r\n        goal.pose.position.y = target_location[1]\r\n        goal.pose.position.z = target_location[2] if len(target_location) > 2 else 0.0\r\n        goal.pose.orientation.w = 1.0\r\n        \r\n        self.navigation_pub.publish(goal)\r\n        \r\n    def execute_manipulation(self):\r\n        """Execute manipulation action"""\r\n        cmd_msg = String()\r\n        cmd_msg.data = "perform_manipulation"\r\n        self.action_pub.publish(cmd_msg)\r\n        \r\n    def execute_return_home(self):\r\n        """Return to home position"""\r\n        goal = PoseStamped()\r\n        goal.header.stamp = self.get_clock().now().to_msg()\r\n        goal.header.frame_id = "map"\r\n        goal.pose.position.x = 0.0\r\n        goal.pose.position.y = 0.0\r\n        goal.pose.position.z = 0.0\r\n        goal.pose.orientation.w = 1.0\r\n        \r\n        self.navigation_pub.publish(goal)\n'})}),"\n",(0,t.jsx)(e.h2,{id:"capstone-project-the-autonomous-humanoid",children:"Capstone Project: The Autonomous Humanoid"}),"\n",(0,t.jsx)(e.p,{children:"For the capstone project, we'll integrate all the concepts learned in the previous modules to create an autonomous humanoid robot that can receive voice commands, understand them, navigate to locations, manipulate objects, and respond to its environment."}),"\n",(0,t.jsx)(e.h3,{id:"system-architecture",children:"System Architecture"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{children:'Voice Command ("Clean the room")\r\n    \u2193\r\n[OpenAI Whisper] \u2192 Natural Language\r\n    \u2193\r\n[LLM Cognitive Planner] \u2192 Action Sequence\r\n    \u2193\r\n[ROS 2 Action Execution]\r\n    \u251c\u2500\u2500 [Isaac Sim] \u2192 Physics Simulation\r\n    \u251c\u2500\u2500 [Navigation] \u2192 Path Planning\r\n    \u251c\u2500\u2500 [Manipulation] \u2192 Object Handling\r\n    \u2514\u2500\u2500 [Perception] \u2192 Environment Awareness\r\n    \u2193\r\nPhysical Humanoid Robot\n'})}),"\n",(0,t.jsx)(e.h3,{id:"complete-autonomous-humanoid-node",children:"Complete Autonomous Humanoid Node"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom geometry_msgs.msg import PoseStamped, Twist\r\nfrom sensor_msgs.msg import Image, LaserScan\r\nfrom nav_msgs.msg import Odometry\r\nfrom cv_bridge import CvBridge\r\nimport threading\r\nimport time\r\nimport openai\r\nimport whisper\r\nimport json\r\n\r\nclass AutonomousHumanoid(Node):\r\n    def __init__(self):\r\n        super().__init__(\'autonomous_humanoid\')\r\n        \r\n        # Initialize interfaces\r\n        self.bridge = CvBridge()\r\n        \r\n        # Voice command interface\r\n        self.voice_command_sub = self.create_subscription(\r\n            String,\r\n            \'/voice_command\',\r\n            self.voice_command_callback,\r\n            10\r\n        )\r\n        \r\n        # Perception interfaces\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            \'/camera/rgb/image_raw\',\r\n            self.image_callback,\r\n            10\r\n        )\r\n        \r\n        self.scan_sub = self.create_subscription(\r\n            LaserScan,\r\n            \'/scan\',\r\n            self.scan_callback,\r\n            10\r\n        )\r\n        \r\n        self.odom_sub = self.create_subscription(\r\n            Odometry,\r\n            \'/odom\',\r\n            self.odom_callback,\r\n            10\r\n        )\r\n        \r\n        # Action interfaces\r\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/cmd_vel\', 10)\r\n        self.navigation_pub = self.create_publisher(PoseStamped, \'/move_base_simple/goal\', 10)\r\n        self.speech_pub = self.create_publisher(String, \'/speech_output\', 10)\r\n        \r\n        # State variables\r\n        self.current_image = None\r\n        self.current_scan = None\r\n        self.current_odom = None\r\n        self.is_executing = False\r\n        \r\n        # Initialize AI components\r\n        self.whisper_model = whisper.load_model("base")\r\n        self.llm_client = self.initialize_llm_client()\r\n        \r\n        # Create action servers\r\n        self.action_lock = threading.Lock()\r\n        \r\n        self.get_logger().info("Autonomous Humanoid system initialized")\r\n    \r\n    def initialize_llm_client(self):\r\n        """Initialize LLM client for cognitive planning"""\r\n        # In a real implementation, this would connect to your preferred LLM\r\n        # For this example, we\'ll use a mock implementation\r\n        class MockLLMClient:\r\n            def plan_action_sequence(self, command, image_description):\r\n                # This would call the actual LLM\r\n                if "clean" in command.lower():\r\n                    return {\r\n                        "actions": [\r\n                            {"type": "scan", "description": "Scan the environment for objects to clean"},\r\n                            {"type": "navigate", "params": {"x": 1.0, "y": 0.5}, "description": "Move to first object location"},\r\n                            {"type": "manipulate", "description": "Pick up object"},\r\n                            {"type": "navigate", "params": {"x": 0.0, "y": 0.0}, "description": "Return to start position"},\r\n                            {"type": "manipulate", "description": "Place object in designated area"}\r\n                        ]\r\n                    }\r\n                elif "bring" in command.lower() or "get" in command.lower():\r\n                    return {\r\n                        "actions": [\r\n                            {"type": "identify", "description": "Identify the requested object"},\r\n                            {"type": "navigate", "params": {"x": 2.0, "y": 1.0}, "description": "Move to object location"},\r\n                            {"type": "manipulate", "description": "Grasp the object"},\r\n                            {"type": "navigate", "params": {"x": 0.0, "y": 0.0}, "description": "Return to user"},\r\n                            {"type": "manipulate", "description": "Release the object to user"}\r\n                        ]\r\n                    }\r\n                else:\r\n                    return {\r\n                        "actions": [\r\n                            {"type": "respond", "params": {"text": "I\'m not sure how to perform that task."}}\r\n                        ]\r\n                    }\r\n        \r\n        return MockLLMClient()\r\n    \r\n    def voice_command_callback(self, msg):\r\n        """Process voice command"""\r\n        command = msg.data\r\n        self.get_logger().info(f"Received voice command: {command}")\r\n        \r\n        # Use a lock to ensure only one command is processed at a time\r\n        if self.action_lock.acquire(blocking=False):\r\n            try:\r\n                self.process_command(command)\r\n            finally:\r\n                self.action_lock.release()\r\n        else:\r\n            self.get_logger().warn("Command received while executing previous command, ignoring.")\r\n    \r\n    def image_callback(self, msg):\r\n        """Process image data"""\r\n        self.current_image = self.bridge.imgmsg_to_cv2(msg, "bgr8")\r\n    \r\n    def scan_callback(self, msg):\r\n        """Process laser scan data"""\r\n        self.current_scan = msg\r\n    \r\n    def odom_callback(self, msg):\r\n        """Process odometry data"""\r\n        self.current_odom = msg\r\n    \r\n    def process_command(self, command):\r\n        """Process a command with cognitive planning and execution"""\r\n        if self.is_executing:\r\n            self.get_logger().warn("Currently executing, cannot process new command")\r\n            return\r\n            \r\n        self.is_executing = True\r\n        self.get_logger().info(f"Processing command: {command}")\r\n        \r\n        try:\r\n            # If we have an image, use it for visual context\r\n            image_context = "Visual information is available" if self.current_image is not None else "No visual information"\r\n            \r\n            # Plan actions using LLM\r\n            action_plan = self.llm_client.plan_action_sequence(command, image_context)\r\n            \r\n            self.get_logger().info(f"Action plan: {action_plan}")\r\n            \r\n            # Execute the action sequence\r\n            for action in action_plan["actions"]:\r\n                self.execute_action(action)\r\n                \r\n                # Small delay between actions\r\n                time.sleep(0.5)\r\n                \r\n            # Report completion\r\n            completion_msg = String()\r\n            completion_msg.data = f"Completed command: {command}"\r\n            self.speech_pub.publish(completion_msg)\r\n            \r\n        except Exception as e:\r\n            self.get_logger().error(f"Error processing command: {e}")\r\n            error_msg = String()\r\n            error_msg.data = f"Error executing command: {e}"\r\n            self.speech_pub.publish(error_msg)\r\n        finally:\r\n            self.is_executing = False\r\n    \r\n    def execute_action(self, action):\r\n        """Execute a single action from the planned sequence"""\r\n        action_type = action["type"]\r\n        description = action.get("description", "No description")\r\n        \r\n        self.get_logger().info(f"Executing action: {action_type} - {description}")\r\n        \r\n        if action_type == "navigate":\r\n            params = action.get("params", {})\r\n            x = params.get("x", 0.0)\r\n            y = params.get("y", 0.0)\r\n            self.execute_navigation(x, y)\r\n            \r\n        elif action_type == "manipulate":\r\n            self.execute_manipulation()\r\n            \r\n        elif action_type == "scan":\r\n            self.execute_scan()\r\n            \r\n        elif action_type == "identify":\r\n            self.execute_identify()\r\n            \r\n        elif action_type == "respond":\r\n            params = action.get("params", {})\r\n            response_text = params.get("text", "Operation complete")\r\n            response_msg = String()\r\n            response_msg.data = response_text\r\n            self.speech_pub.publish(response_msg)\r\n    \r\n    def execute_navigation(self, x, y):\r\n        """Execute navigation to specified coordinates"""\r\n        goal = PoseStamped()\r\n        goal.header.stamp = self.get_clock().now().to_msg()\r\n        goal.header.frame_id = "map"\r\n        goal.pose.position.x = float(x)\r\n        goal.pose.position.y = float(y)\r\n        goal.pose.position.z = 0.0\r\n        goal.pose.orientation.w = 1.0\r\n        \r\n        self.navigation_pub.publish(goal)\r\n        \r\n        # Wait for navigation to complete (simplified)\r\n        time.sleep(3)  # In a real system, wait for navigation feedback\r\n    \r\n    def execute_manipulation(self):\r\n        """Execute manipulation action"""\r\n        # In a real system, this would publish to manipulation controller\r\n        # For now, just log the action\r\n        self.get_logger().info("Executing manipulation action")\r\n        \r\n        # In a real humanoid, this might involve:\r\n        # - Planning arm trajectory\r\n        # - Controlling joint positions\r\n        # - Managing grasping forces\r\n        # For now, just simulate the action\r\n        time.sleep(2)\r\n    \r\n    def execute_scan(self):\r\n        """Execute environment scanning"""\r\n        # Publish command to perception system to scan environment\r\n        scan_cmd = String()\r\n        scan_cmd.data = "scan_environment"\r\n        self.speech_pub.publish(scan_cmd)\r\n        \r\n        # Simulate scan time\r\n        time.sleep(1)\r\n    \r\n    def execute_identify(self):\r\n        """Execute object identification"""\r\n        # In a real system, this would use perception algorithms\r\n        # to identify objects in the environment\r\n        self.get_logger().info("Executing object identification")\r\n        time.sleep(1)\r\n    \r\n    def get_robot_position(self):\r\n        """Get current robot position from odometry"""\r\n        if self.current_odom:\r\n            return (\r\n                self.current_odom.pose.pose.position.x,\r\n                self.current_odom.pose.pose.position.y\r\n            )\r\n        return (0.0, 0.0)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    humanoid_node = AutonomousHumanoid()\r\n    \r\n    try:\r\n        rclpy.spin(humanoid_node)\r\n    except KeyboardInterrupt:\r\n        humanoid_node.get_logger().info("Shutting down Autonomous Humanoid...")\r\n    finally:\r\n        humanoid_node.destroy_node()\r\n        rclpy.shutdown()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\n'})}),"\n",(0,t.jsx)(e.h2,{id:"system-integration-and-testing",children:"System Integration and Testing"}),"\n",(0,t.jsx)(e.p,{children:"To test the complete system, we need a launch file that brings up all components:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-xml",children:"\x3c!-- launch/autonomous_humanoid.launch.py --\x3e\r\nimport launch\r\nfrom launch import LaunchDescription\r\nfrom launch_ros.actions import Node\r\nfrom ament_index_python.packages import get_package_share_directory\r\nimport os\r\n\r\ndef generate_launch_description():\r\n    return LaunchDescription([\r\n        # Voice recognition node (using our Whisper implementation)\r\n        Node(\r\n            package='speech_recognition',\r\n            executable='whisper_node',\r\n            name='whisper_voice_node',\r\n            output='screen'\r\n        ),\r\n        \r\n        # Navigation stack\r\n        Node(\r\n            package='nav2_bringup',\r\n            executable='nav2_launch.py',\r\n            name='navigation2',\r\n            output='screen'\r\n        ),\r\n        \r\n        # Perception stack\r\n        Node(\r\n            package='object_detection',\r\n            executable='object_detector',\r\n            name='object_detector',\r\n            output='screen'\r\n        ),\r\n        \r\n        # Main cognitive planner\r\n        Node(\r\n            package='humanoid_control',\r\n            executable='autonomous_humanoid',\r\n            name='autonomous_humanoid',\r\n            output='screen'\r\n        ),\r\n        \r\n        # Isaac Sim bridge (if running simulation)\r\n        Node(\r\n            package='isaac_ros_bridges',\r\n            executable='ros_bridge',\r\n            name='isaac_ros_bridge',\r\n            output='screen'\r\n        )\r\n    ])\n"})}),"\n",(0,t.jsx)(e.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,t.jsx)(e.p,{children:"When implementing a complete VLA system:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Latency"}),": Minimize delays between command input and action execution"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Reliability"}),": Implement fallback mechanisms for when one component fails"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety"}),": Include safety checks and emergency stops"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Resource Management"}),": Optimize GPU and CPU usage across all components"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Robustness"}),": Handle ambiguous commands and unexpected situations"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"evaluation-metrics",children:"Evaluation Metrics"}),"\n",(0,t.jsx)(e.p,{children:"To evaluate the autonomous humanoid system:"}),"\n",(0,t.jsxs)(e.ol,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Task Completion Rate"}),": Percentage of tasks successfully completed"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Command Understanding Accuracy"}),": How often the LLM correctly interprets commands"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Navigation Success Rate"}),": Percentage of successful navigation attempts"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Response Time"}),": Average time from command to first action"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Safety Incidents"}),": Number of collisions or unsafe behaviors"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"conclusion",children:"Conclusion"}),"\n",(0,t.jsx)(e.p,{children:"This capstone project combines all the concepts from the previous modules:"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Module 1 (ROS 2)"}),": Provides the communication backbone and node architecture"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Module 2 (Gazebo/Unity)"}),": Enables simulation and testing in virtual environments"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Module 3 (Isaac)"}),": Provides AI processing and perception capabilities"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Module 4 (VLA)"}),": Integrates voice, vision, and action for natural interaction"]}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"The autonomous humanoid represents a complete robotic system where natural language commands are understood, planned, and executed in the physical world. This integration demonstrates the power of combining multiple advanced technologies to create capable, intelligent robotic systems."})]})}function d(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(m,{...n})}):m(n)}},8453:(n,e,r)=>{r.d(e,{R:()=>a,x:()=>s});var o=r(6540);const t={},i=o.createContext(t);function a(n){const e=o.useContext(i);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:a(n.components),o.createElement(i.Provider,{value:e},n.children)}}}]);
"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[7702],{2957:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"modules/module-4-vla/lesson-2-voice-to-action","title":"Lesson 2: Voice-to-Action with OpenAI Whisper for Robotic Control","description":"Introduction to Voice Commands in Robotics","source":"@site/docs/modules/module-4-vla/lesson-2-voice-to-action.md","sourceDirName":"modules/module-4-vla","slug":"/modules/module-4-vla/lesson-2-voice-to-action","permalink":"/AI_Book_Hackathon/docs/modules/module-4-vla/lesson-2-voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/mussawirsohail/AI_Book_Hackathon/docs/modules/module-4-vla/lesson-2-voice-to-action.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2}}');var i=n(4848),t=n(8453);const s={sidebar_position:2},a="Lesson 2: Voice-to-Action with OpenAI Whisper for Robotic Control",c={},l=[{value:"Introduction to Voice Commands in Robotics",id:"introduction-to-voice-commands-in-robotics",level:2},{value:"OpenAI Whisper Overview",id:"openai-whisper-overview",level:2},{value:"Whisper in Robotics Context",id:"whisper-in-robotics-context",level:3},{value:"Local Whisper Deployment for Privacy and Latency",id:"local-whisper-deployment-for-privacy-and-latency",level:2},{value:"Speech Processing with VAD (Voice Activity Detection)",id:"speech-processing-with-vad-voice-activity-detection",level:2},{value:"Integration with ROS 2 Speech Recognition Interface",id:"integration-with-ros-2-speech-recognition-interface",level:2},{value:"Advanced: Wake Word Detection",id:"advanced-wake-word-detection",level:2},{value:"Performance Considerations",id:"performance-considerations",level:2}];function d(e){const r={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(r.header,{children:(0,i.jsx)(r.h1,{id:"lesson-2-voice-to-action-with-openai-whisper-for-robotic-control",children:"Lesson 2: Voice-to-Action with OpenAI Whisper for Robotic Control"})}),"\n",(0,i.jsx)(r.h2,{id:"introduction-to-voice-commands-in-robotics",children:"Introduction to Voice Commands in Robotics"}),"\n",(0,i.jsx)(r.p,{children:"Natural voice commands provide an intuitive way for humans to interact with robots. Using OpenAI Whisper, a state-of-the-art speech recognition model, robots can understand and respond to spoken commands in real-time."}),"\n",(0,i.jsx)(r.h2,{id:"openai-whisper-overview",children:"OpenAI Whisper Overview"}),"\n",(0,i.jsx)(r.p,{children:"Whisper is a general-purpose speech recognition model that can transcribe audio in multiple languages. For robotics applications, Whisper can be used to convert voice commands into text that can be processed by robot reasoning systems."}),"\n",(0,i.jsx)(r.h3,{id:"whisper-in-robotics-context",children:"Whisper in Robotics Context"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nfrom audio_common_msgs.msg import AudioData\r\nimport openai\r\nimport numpy as np\r\nimport pyaudio\r\nimport wave\r\nimport tempfile\r\nimport threading\r\nfrom io import BytesIO\r\n\r\nclass WhisperVoiceNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'whisper_voice_node\')\r\n        \r\n        # Initialize Whisper client\r\n        # This assumes you have access to OpenAI API or are using a locally deployed version\r\n        self.whisper_client = openai.OpenAI()  # For OpenAI API\r\n        \r\n        # Audio recording parameters\r\n        self.audio_format = pyaudio.paInt16\r\n        self.channels = 1\r\n        self.rate = 16000\r\n        self.chunk = 1024\r\n        self.record_seconds = 5\r\n        \r\n        # Publisher for transcribed commands\r\n        self.command_pub = self.create_publisher(String, \'/robot_command\', 10)\r\n        \r\n        # Initialize audio interface\r\n        self.audio_interface = pyaudio.PyAudio()\r\n        \r\n        # Start recording thread\r\n        self.recording = False\r\n        self.record_thread = threading.Thread(target=self.continuous_recording)\r\n        self.record_thread.start()\r\n        \r\n    def continuous_recording(self):\r\n        """Continuously record audio and send to Whisper for transcription"""\r\n        while rclpy.ok():\r\n            # Record audio\r\n            frames = []\r\n            \r\n            stream = self.audio_interface.open(\r\n                format=self.audio_format,\r\n                channels=self.channels,\r\n                rate=self.rate,\r\n                input=True,\r\n                frames_per_buffer=self.chunk\r\n            )\r\n            \r\n            # Record for specified duration\r\n            for i in range(0, int(self.rate / self.chunk * self.record_seconds)):\r\n                data = stream.read(self.chunk)\r\n                frames.append(data)\r\n            \r\n            # Stop recording\r\n            stream.stop_stream()\r\n            stream.close()\r\n            \r\n            # Convert recorded audio to WAV format\r\n            audio_data = self.save_audio_as_wav(frames)\r\n            \r\n            # Transcribe using Whisper\r\n            try:\r\n                transcription = self.transcribe_audio(audio_data)\r\n                \r\n                if transcription.strip():\r\n                    self.publish_command(transcription)\r\n                    \r\n            except Exception as e:\r\n                self.get_logger().error(f"Error in transcription: {e}")\r\n                \r\n            # Sleep before next recording\r\n            self.get_clock().sleep_for(rclpy.duration.Duration(seconds=1))\r\n    \r\n    def save_audio_as_wav(self, frames):\r\n        """Save recorded audio frames to a temporary WAV file"""\r\n        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\'.wav\')\r\n        \r\n        with wave.open(temp_file.name, \'wb\') as wf:\r\n            wf.setnchannels(self.channels)\r\n            wf.setsampwidth(self.audio_interface.get_sample_size(self.audio_format))\r\n            wf.setframerate(self.rate)\r\n            wf.writeframes(b\'\'.join(frames))\r\n            \r\n        return temp_file.name\r\n    \r\n    def transcribe_audio(self, audio_file_path):\r\n        """Transcribe audio file using OpenAI Whisper API"""\r\n        with open(audio_file_path, "rb") as audio_file:\r\n            response = self.whisper_client.audio.transcriptions.create(\r\n                model="whisper-1",\r\n                file=audio_file\r\n            )\r\n        \r\n        return response.text\r\n    \r\n    def publish_command(self, transcription):\r\n        """Publish transcribed command to robot command topic"""\r\n        if self.is_robot_command(transcription):\r\n            cmd_msg = String()\r\n            cmd_msg.data = transcription\r\n            self.command_pub.publish(cmd_msg)\r\n            self.get_logger().info(f"Published command: {transcription}")\r\n    \r\n    def is_robot_command(self, text):\r\n        """Check if the transcribed text contains robot commands"""\r\n        # Simple heuristic - in practice, this could be more sophisticated\r\n        robot_keywords = [\r\n            \'move\', \'go\', \'stop\', \'turn\', \'pick\', \'place\', \'clean\',\r\n            \'bring\', \'take\', \'grasp\', \'navigate\', \'help\', \'come\'\r\n        ]\r\n        \r\n        text_lower = text.lower()\r\n        return any(keyword in text_lower for keyword in robot_keywords)\r\n\r\ndef main(args=None):\r\n    rclpy.init(args=args)\r\n    node = WhisperVoiceNode()\r\n    \r\n    try:\r\n        rclpy.spin(node)\r\n    except KeyboardInterrupt:\r\n        node.get_logger().info("Shutting down Whisper voice node...")\r\n    finally:\r\n        node.audio_interface.terminate()\r\n        node.destroy_node()\r\n        rclpy.shutdown()\n'})}),"\n",(0,i.jsx)(r.h2,{id:"local-whisper-deployment-for-privacy-and-latency",children:"Local Whisper Deployment for Privacy and Latency"}),"\n",(0,i.jsx)(r.p,{children:"For real-time applications and privacy concerns, it's often better to deploy Whisper locally:"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import rclpy\r\nfrom rclpy.node import Node\r\nfrom std_msgs.msg import String\r\nimport torch\r\nimport whisper\r\nimport pyaudio\r\nimport wave\r\nimport numpy as np\r\nimport tempfile\r\n\r\nclass LocalWhisperNode(Node):\r\n    def __init__(self):\r\n        super().__init__(\'local_whisper_node\')\r\n        \r\n        # Load Whisper model locally\r\n        # Options: tiny, base, small, medium, large\r\n        self.model = whisper.load_model("small")\r\n        \r\n        # Audio recording parameters\r\n        self.audio_format = pyaudio.paInt16\r\n        self.channels = 1\r\n        self.rate = 16000\r\n        self.chunk = 1024\r\n        self.record_seconds = 5\r\n        \r\n        # Publisher for transcribed commands\r\n        self.command_pub = self.create_publisher(String, \'/robot_command\', 10)\r\n        \r\n        # Initialize audio interface\r\n        self.audio_interface = pyaudio.PyAudio()\r\n        \r\n        self.get_logger().info("Local Whisper node initialized with small model")\r\n    \r\n    def transcribe_audio_local(self, audio_file_path):\r\n        """Transcribe audio file using local Whisper model"""\r\n        result = self.model.transcribe(audio_file_path)\r\n        return result["text"]\r\n    \r\n    def record_and_transcribe(self):\r\n        """Record audio and transcribe it locally"""\r\n        frames = []\r\n        \r\n        stream = self.audio_interface.open(\r\n            format=self.audio_format,\r\n            channels=self.channels,\r\n            rate=self.rate,\r\n            input=True,\r\n            frames_per_buffer=self.chunk\r\n        )\r\n        \r\n        self.get_logger().info("Recording...")\r\n        \r\n        # Record audio\r\n        for i in range(0, int(self.rate / self.chunk * self.record_seconds)):\r\n            data = stream.read(self.chunk)\r\n            frames.append(data)\r\n        \r\n        # Stop recording\r\n        stream.stop_stream()\r\n        stream.close()\r\n        \r\n        # Save to temporary file\r\n        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\'.wav\')\r\n        with wave.open(temp_file.name, \'wb\') as wf:\r\n            wf.setnchannels(self.channels)\r\n            wf.setsampwidth(self.audio_interface.get_sample_size(self.audio_format))\r\n            wf.setframerate(self.rate)\r\n            wf.writeframes(b\'\'.join(frames))\r\n        \r\n        # Transcribe\r\n        transcription = self.transcribe_audio_local(temp_file.name)\r\n        return transcription\n'})}),"\n",(0,i.jsx)(r.h2,{id:"speech-processing-with-vad-voice-activity-detection",children:"Speech Processing with VAD (Voice Activity Detection)"}),"\n",(0,i.jsx)(r.p,{children:"Using Voice Activity Detection (VAD) improves the efficiency of voice processing by only capturing when someone is talking:"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:"import webrtcvad\r\nimport collections\r\n\r\nclass VADWhisperNode(Node):\r\n    def __init__(self):\r\n        super().__init__('vad_whisper_node')\r\n        \r\n        # Initialize Whisper model\r\n        self.model = whisper.load_model(\"tiny\")\r\n        \r\n        # VAD parameters\r\n        self.vad = webrtcvad.Vad()\r\n        self.vad.set_mode(3)  # Aggressive mode\r\n        \r\n        # Audio parameters\r\n        self.audio_format = pyaudio.paInt16\r\n        self.channels = 1\r\n        self.rate = 16000\r\n        self.chunk_duration = 30  # ms (must be 10, 20, or 30)\r\n        self.chunk_size = int(self.rate * self.chunk_duration / 1000)\r\n        \r\n        # Ring buffer to hold audio frames\r\n        self.ring_buffer = collections.deque(maxlen=int(30 * self.rate / 1000))  # 30 seconds buffer\r\n        \r\n        # Publisher for commands\r\n        self.command_pub = self.create_publisher(String, '/robot_command', 10)\r\n        \r\n        # Audio interface\r\n        self.audio_interface = pyaudio.PyAudio()\r\n        \r\n        # Start listening\r\n        self.start_listening()\r\n    \r\n    def start_listening(self):\r\n        stream = self.audio_interface.open(\r\n            format=self.audio_format,\r\n            channels=self.channels,\r\n            rate=self.rate,\r\n            input=True,\r\n            frames_per_buffer=self.chunk_size\r\n        )\r\n        \r\n        voiced_frames = []\r\n        triggered = False\r\n        \r\n        while rclpy.ok():\r\n            # Read audio chunk\r\n            chunk = stream.read(self.chunk_size)\r\n            \r\n            # Convert to raw format for VAD\r\n            is_speech = self.vad.is_speech(chunk, self.rate)\r\n            \r\n            if not triggered:\r\n                # Not in a voice activity state\r\n                if is_speech:\r\n                    # Voice activity detected\r\n                    triggered = True\r\n                    voiced_frames.extend(self.ring_buffer)  # Add buffered frames\r\n                    voiced_frames.append(chunk)\r\n                    self.get_logger().info(\"Voice activity detected\")\r\n            else:\r\n                # In a voice activity state\r\n                if is_speech:\r\n                    # Continue to add frames\r\n                    voiced_frames.append(chunk)\r\n                else:\r\n                    # Voice activity ended\r\n                    if len(voiced_frames) > 1:  # Only process if we have meaningful speech\r\n                        self.process_voice_command(b''.join(voiced_frames))\r\n                    \r\n                    triggered = False\r\n                    voiced_frames = []\r\n            \r\n            # Add chunk to ring buffer\r\n            self.ring_buffer.append(chunk)\r\n    \r\n    def process_voice_command(self, audio_data):\r\n        \"\"\"Process recorded voice command\"\"\"\r\n        # Convert raw audio to WAV\r\n        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.wav')\r\n        \r\n        with wave.open(temp_file.name, 'wb') as wf:\r\n            wf.setnchannels(self.channels)\r\n            wf.setsampwidth(self.audio_interface.get_sample_size(self.audio_format))\r\n            wf.setframerate(self.rate)\r\n            wf.writeframes(audio_data)\r\n        \r\n        # Transcribe\r\n        try:\r\n            result = self.model.transcribe(temp_file.name)\r\n            transcription = result[\"text\"]\r\n            \r\n            if transcription.strip():\r\n                self.get_logger().info(f\"Transcribed: {transcription}\")\r\n                \r\n                # Check if it's a robot command\r\n                if self.is_robot_command(transcription):\r\n                    cmd_msg = String()\r\n                    cmd_msg.data = transcription\r\n                    self.command_pub.publish(cmd_msg)\r\n                    \r\n        except Exception as e:\r\n            self.get_logger().error(f\"Error during transcription: {e}\")\r\n    \r\n    def is_robot_command(self, text):\r\n        \"\"\"Simple command detection\"\"\"\r\n        robot_keywords = [\r\n            'move', 'go', 'stop', 'turn', 'pickup', 'place', 'clean',\r\n            'bring', 'take', 'get', 'help', 'come here', 'follow me'\r\n        ]\r\n        \r\n        text_lower = text.lower()\r\n        return any(keyword in text_lower for keyword in robot_keywords)\n"})}),"\n",(0,i.jsx)(r.h2,{id:"integration-with-ros-2-speech-recognition-interface",children:"Integration with ROS 2 Speech Recognition Interface"}),"\n",(0,i.jsx)(r.p,{children:"To follow ROS 2 best practices, let's create a proper speech recognition action server:"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'from rclpy.action import ActionServer\r\nfrom rclpy.callback_groups import ReentrantCallbackGroup\r\nfrom rclpy.executors import MultiThreadedExecutor\r\nfrom std_msgs.msg import String\r\nfrom audio_common_msgs.msg import AudioData\r\nfrom speech_recognition_msgs.action import RecognizeSpeech\r\nimport threading\r\n\r\nclass SpeechRecognitionActionServer(Node):\r\n    def __init__(self):\r\n        super().__init__(\'speech_recognition_action_server\')\r\n        \r\n        # Initialize Whisper model\r\n        self.model = whisper.load_model("base")\r\n        \r\n        # Use reentrant callback group since we\'ll have concurrent operations\r\n        self.callback_group = ReentrantCallbackGroup()\r\n        \r\n        # Create action server\r\n        self._action_server = ActionServer(\r\n            self,\r\n            RecognizeSpeech,\r\n            \'recognize_speech\',\r\n            self.execute_callback,\r\n            callback_group=self.callback_group\r\n        )\r\n        \r\n        # Publisher for recognized text\r\n        self.recognized_text_pub = self.create_publisher(String, \'/recognized_text\', 10)\r\n        \r\n        self.get_logger().info("Speech Recognition Action Server ready")\r\n    \r\n    def execute_callback(self, goal_handle):\r\n        """Execute the speech recognition action"""\r\n        self.get_logger().info(\'Executing speech recognition goal\')\r\n        \r\n        # Get audio data from goal\r\n        audio_data = goal_handle.request.audio\r\n        \r\n        # Transcribe the audio\r\n        try:\r\n            # Convert ROS AudioData to appropriate format for Whisper\r\n            transcription = self.transcribe_audio_data(audio_data)\r\n            \r\n            # Create result\r\n            result = RecognizeSpeech.Result()\r\n            result.transcript = transcription\r\n            result.confidence = 1.0  # Placeholder - in real system, use model\'s confidence\r\n            \r\n            # Publish the recognized text\r\n            text_msg = String()\r\n            text_msg.data = transcription\r\n            self.recognized_text_pub.publish(text_msg)\r\n            \r\n            goal_handle.succeed()\r\n            return result\r\n            \r\n        except Exception as e:\r\n            self.get_logger().error(f"Error in speech recognition: {e}")\r\n            goal_handle.abort()\r\n            result = RecognizeSpeech.Result()\r\n            result.transcript = ""\r\n            result.confidence = 0.0\r\n            return result\r\n    \r\n    def transcribe_audio_data(self, audio_data_msg):\r\n        """Transcribe ROS AudioData message using Whisper"""\r\n        # This would convert the ROS audio message format to something\r\n        # Whisper can process - implementation would depend on the audio format\r\n        # and might require additional audio processing libraries\r\n        \r\n        # For now, showing the general approach\r\n        # Convert audio_data_msg.data to WAV format\r\n        # with appropriate sample rate and encoding\r\n        pass\n'})}),"\n",(0,i.jsx)(r.h2,{id:"advanced-wake-word-detection",children:"Advanced: Wake Word Detection"}),"\n",(0,i.jsx)(r.p,{children:"For always-listening systems, it's important to implement wake word detection:"}),"\n",(0,i.jsx)(r.pre,{children:(0,i.jsx)(r.code,{className:"language-python",children:'import collections\r\nimport numpy as np\r\n\r\nclass WakeWordDetector:\r\n    def __init__(self, wake_words=["robot", "hey robot", "assistant"]):\r\n        self.wake_words = wake_words\r\n        self.audio_buffer = collections.deque(maxlen=40000)  # 1 second buffer at 16kHz\r\n        \r\n    def is_wake_word_spotted(self, audio_chunk):\r\n        """Detect if a wake word is present in the audio"""\r\n        # This is a simplified approach\r\n        # In practice, you\'d use a dedicated wake word detection model like Porcupine\r\n        # or implement keyword spotting using machine learning\r\n        \r\n        # Add chunk to buffer\r\n        self.audio_buffer.extend(audio_chunk)\r\n        \r\n        # Convert to text and check for wake words\r\n        # This is a placeholder - real implementation would use a lightweight\r\n        # keyword detection model\r\n        return False  # Placeholder return\n'})}),"\n",(0,i.jsx)(r.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,i.jsx)(r.p,{children:"When implementing Whisper-based systems:"}),"\n",(0,i.jsxs)(r.ol,{children:["\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Model Size"}),": Larger models are more accurate but slower. Choose based on your latency requirements."]}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Latency vs. Accuracy"}),": Consider using faster models for real-time applications."]}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Hardware Acceleration"}),": Use GPU acceleration if available to speed up processing."]}),"\n"]}),"\n",(0,i.jsxs)(r.li,{children:["\n",(0,i.jsxs)(r.p,{children:[(0,i.jsx)(r.strong,{children:"Offline vs. Online"}),": Local models eliminate network dependency and improve privacy."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(r.p,{children:["Continue to ",(0,i.jsx)(r.a,{href:"/AI_Book_Hackathon/docs/modules/module-4-vla/lesson-3-cognitive-planning",children:"Lesson 3: Cognitive Planning with LLMs"})," to learn how to use Large Language Models for translating natural language commands into sequences of robotic actions."]})]})}function p(e={}){const{wrapper:r}={...(0,t.R)(),...e.components};return r?(0,i.jsx)(r,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,r,n)=>{n.d(r,{R:()=>s,x:()=>a});var o=n(6540);const i={},t=o.createContext(i);function s(e){const r=o.useContext(t);return o.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function a(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(t.Provider,{value:r},e.children)}}}]);
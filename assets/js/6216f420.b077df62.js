"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[9630],{8453:(e,r,n)=>{n.d(r,{R:()=>a,x:()=>i});var t=n(6540);const s={},o=t.createContext(s);function a(e){const r=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function i(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(o.Provider,{value:r},e.children)}},9709:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>c,contentTitle:()=>i,default:()=>l,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"modules/module-5-rag-chatbot/lesson-4-rag-backend","title":"Lesson 4: Building the RAG Backend with FastAPI","description":"Introduction","source":"@site/docs/modules/module-5-rag-chatbot/lesson-4-rag-backend.md","sourceDirName":"modules/module-5-rag-chatbot","slug":"/modules/module-5-rag-chatbot/lesson-4-rag-backend","permalink":"/AI_Book_Hackathon/docs/modules/module-5-rag-chatbot/lesson-4-rag-backend","draft":false,"unlisted":false,"editUrl":"https://github.com/mussawirsohail/AI_Book_Hackathon/edit/main/docs/modules/module-5-rag-chatbot/lesson-4-rag-backend.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 3: Vector Database Setup with Qdrant","permalink":"/AI_Book_Hackathon/docs/modules/module-5-rag-chatbot/lesson-3-vector-database"},"next":{"title":"Lesson 5: Creating the Professional Frontend UI","permalink":"/AI_Book_Hackathon/docs/modules/module-5-rag-chatbot/lesson-5-frontend-ui"}}');var s=n(4848),o=n(8453);const a={sidebar_position:4},i="Lesson 4: Building the RAG Backend with FastAPI",c={},d=[{value:"Introduction",id:"introduction",level:2},{value:"FastAPI Application Structure",id:"fastapi-application-structure",level:2},{value:"RAG Core Service",id:"rag-core-service",level:2},{value:"Chat API Endpoint",id:"chat-api-endpoint",level:2},{value:"API Dependencies and Security",id:"api-dependencies-and-security",level:2},{value:"Configuration Management",id:"configuration-management",level:2},{value:"Database Models Integration",id:"database-models-integration",level:2},{value:"Testing the Backend",id:"testing-the-backend",level:2}];function u(e){const r={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",p:"p",pre:"pre",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.header,{children:(0,s.jsx)(r.h1,{id:"lesson-4-building-the-rag-backend-with-fastapi",children:"Lesson 4: Building the RAG Backend with FastAPI"})}),"\n",(0,s.jsx)(r.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(r.p,{children:"In this lesson, we'll build the core backend functionality for our RAG chatbot using FastAPI. This includes implementing the retrieval logic that fetches relevant documents from our vector database and the generation logic that creates responses based on the retrieved context."}),"\n",(0,s.jsx)(r.h2,{id:"fastapi-application-structure",children:"FastAPI Application Structure"}),"\n",(0,s.jsx)(r.p,{children:"Let's start with the main application setup:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'# backend/app/main.py\r\nfrom fastapi import FastAPI\r\nfrom fastapi.middleware.cors import CORSMiddleware\r\nfrom app.api.v1.api import api_router\r\nfrom app.config import settings\r\nimport logging\r\n\r\n# Set up logging\r\nlogging.basicConfig(level=logging.INFO)\r\nlogger = logging.getLogger(__name__)\r\n\r\napp = FastAPI(\r\n    title="RAG Chatbot API",\r\n    description="API for Retrieval-Augmented Generation chatbot",\r\n    version="1.0.0",\r\n    openapi_url="/api/v1/openapi.json"\r\n)\r\n\r\n# Add CORS middleware\r\napp.add_middleware(\r\n    CORSMiddleware,\r\n    allow_origins=settings.ALLOWED_ORIGINS,\r\n    allow_credentials=True,\r\n    allow_methods=["*"],\r\n    allow_headers=["*"],\r\n    # Expose headers for authentication\r\n    expose_headers=["Access-Control-Allow-Origin"]\r\n)\r\n\r\n# Include API routes\r\napp.include_router(api_router, prefix="/api/v1")\r\n\r\n@app.on_event("startup")\r\nasync def startup_event():\r\n    """Initialize resources on startup"""\r\n    logger.info("Starting up RAG Chatbot API")\r\n    # Initialize vector database connection\r\n    # Initialize LLM clients\r\n    # Load any necessary models\r\n\r\n@app.on_event("shutdown")\r\nasync def shutdown_event():\r\n    """Clean up resources on shutdown"""\r\n    logger.info("Shutting down RAG Chatbot API")\r\n    # Close database connections\r\n    # Clean up resources\r\n\r\n@app.get("/")\r\ndef read_root():\r\n    return {"message": "RAG Chatbot API is running!"}\r\n\r\n@app.get("/health")\r\ndef health_check():\r\n    return {"status": "healthy"}\n'})}),"\n",(0,s.jsx)(r.h2,{id:"rag-core-service",children:"RAG Core Service"}),"\n",(0,s.jsx)(r.p,{children:"Now let's implement the core RAG service that orchestrates retrieval and generation:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'# backend/app/services/rag_service.py\r\nimport asyncio\r\nfrom typing import List, Dict, Any, Optional\r\nfrom app.vector_db import VectorDBManager\r\nfrom app.document_processor import DocumentProcessor\r\nfrom app.config import settings\r\nfrom app.models.user import UserRead\r\nimport openai\r\nimport logging\r\n\r\nclass RAGService:\r\n    def __init__(self):\r\n        self.vector_db = VectorDBManager(\r\n            url=settings.QDRANT_URL, \r\n            api_key=settings.QDRANT_API_KEY\r\n        )\r\n        self.doc_processor = DocumentProcessor()\r\n        self.openai_client = openai.AsyncOpenAI(api_key=settings.OPENAI_API_KEY)\r\n        \r\n    async def generate_response(\r\n        self, \r\n        query: str, \r\n        user_background: Optional[Dict[str, str]] = None,\r\n        user_id: Optional[str] = None,\r\n        selected_text: Optional[str] = None\r\n    ) -> Dict[str, Any]:\r\n        """\r\n        Generate a response using RAG approach\r\n        \r\n        Args:\r\n            query: User\'s question\r\n            user_background: User\'s software/hardware background for personalization\r\n            user_id: ID of the requesting user\r\n            selected_text: Optional text provided by the user to restrict answers to\r\n        \r\n        Returns:\r\n            Dictionary containing response and metadata\r\n        """\r\n        try:\r\n            # 1. If selected text is provided, use only that as context\r\n            context_texts = []\r\n            if selected_text:\r\n                context_texts = [selected_text]\r\n                sources = []\r\n            else:\r\n                # 2. Generate embedding for the query\r\n                query_embedding = await self.doc_processor.embedding_model.aembed_query(query)\r\n                \r\n                # 3. Search for relevant documents in vector database\r\n                search_results = await self.vector_db.search_similar(\r\n                    query_embedding, \r\n                    top_k=5\r\n                )\r\n                \r\n                # Extract the content from search results\r\n                context_texts = [result["content"] for result in search_results]\r\n                sources = search_results\r\n\r\n            # 4. Create personalized context based on user background\r\n            context = self._create_context_with_user_background(\r\n                query, context_texts, user_background\r\n            )\r\n            \r\n            # 5. Generate response using LLM\r\n            response = await self._generate_llm_response(context, query)\r\n            \r\n            # 6. Create response with metadata\r\n            result = {\r\n                "response": response,\r\n                "sources": sources,\r\n                "query": query,\r\n                "model_used": "gpt-3.5-turbo"  # or whatever model you\'re using\r\n            }\r\n            \r\n            # 7. Log the interaction\r\n            await self._log_interaction(query, response, user_id, sources)\r\n            \r\n            return result\r\n            \r\n        except Exception as e:\r\n            logging.error(f"Error in RAG generation: {e}")\r\n            raise\r\n    \r\n    def _create_context_with_user_background(\r\n        self, \r\n        query: str, \r\n        context_texts: List[str], \r\n        user_background: Optional[Dict[str, str]]\r\n    ) -> str:\r\n        """Create a context string with user background information for personalization"""\r\n        # Start with background information if available\r\n        background_context = ""\r\n        if user_background:\r\n            software_bg = user_background.get("software_background", "beginner")\r\n            hardware_bg = user_background.get("hardware_background", "beginner")\r\n            \r\n            # Adjust the persona based on user\'s background\r\n            if software_bg == "beginner" or hardware_bg == "beginner":\r\n                background_context = "Explain concepts in simple terms with practical examples. "\r\n            elif software_bg == "expert" or hardware_bg == "expert":\r\n                background_context = "Provide detailed technical explanations with advanced concepts. "\r\n            else:\r\n                background_context = "Provide balanced explanations with appropriate detail. "\r\n        \r\n        # Combine background with the context from documents\r\n        full_context = background_context + "\\n\\n"\r\n        \r\n        if context_texts:\r\n            full_context += "Relevant information from the book:\\n\\n"\r\n            for i, text in enumerate(context_texts):\r\n                full_context += f"Source {i+1}:\\n{text}\\n\\n"\r\n        else:\r\n            full_context += "No relevant information was found in the book. "\r\n            \r\n        return full_context\r\n    \r\n    async def _generate_llm_response(self, context: str, query: str) -> str:\r\n        """Generate a response using the LLM with the provided context"""\r\n        # Create the prompt for the LLM\r\n        prompt = f"""\r\n        You are a helpful assistant for a book. Answer the user\'s question based on the context provided below.\r\n        \r\n        Context: {context}\r\n        \r\n        User\'s question: {query}\r\n        \r\n        Instructions:\r\n        - If the context contains relevant information, use it to answer the question.\r\n        - If the context doesn\'t contain relevant information, state that you don\'t have enough information from the book to answer.\r\n        - Be concise and helpful in your response.\r\n        - Cite sources when possible using the source numbers mentioned in the context.\r\n        """\r\n        \r\n        try:\r\n            response = await self.openai_client.chat.completions.create(\r\n                model="gpt-3.5-turbo",  # You can change this to gpt-4 if preferred\r\n                messages=[\r\n                    {"role": "system", "content": "You are a helpful assistant that answers questions based on provided context from a book."},\r\n                    {"role": "user", "content": prompt}\r\n                ],\r\n                max_tokens=1000,\r\n                temperature=0.3,  # Lower temperature for more consistent responses\r\n            )\r\n            \r\n            return response.choices[0].message.content\r\n        \r\n        except Exception as e:\r\n            logging.error(f"Error generating LLM response: {e}")\r\n            raise\r\n    \r\n    async def _log_interaction(\r\n        self, \r\n        query: str, \r\n        response: str, \r\n        user_id: Optional[str], \r\n        sources: List[Dict]\r\n    ):\r\n        """Log the interaction for analytics and improvement"""\r\n        # This would typically store in a database for analytics\r\n        logging.info(f"Interaction logged - Query: {query[:50]}... | User ID: {user_id}")\n'})}),"\n",(0,s.jsx)(r.h2,{id:"chat-api-endpoint",children:"Chat API Endpoint"}),"\n",(0,s.jsx)(r.p,{children:"Now let's implement the API endpoint that connects to our RAG service:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'# backend/app/api/v1/chat.py\r\nfrom fastapi import APIRouter, Depends, HTTPException, Request\r\nfrom sqlalchemy.ext.asyncio import AsyncSession\r\nfrom app.database import get_async_session\r\nfrom app.models.user import UserRead\r\nfrom app.api.deps import get_current_user\r\nfrom app.services.rag_service import RAGService\r\nfrom pydantic import BaseModel\r\nfrom typing import Optional, List, Dict, Any\r\nimport logging\r\n\r\nrouter = APIRouter()\r\nrag_service = RAGService()\r\n\r\nclass ChatRequest(BaseModel):\r\n    message: str\r\n    selected_text: Optional[str] = None  # Text selected by user to restrict answers to\r\n    session_id: Optional[str] = None     # For multi-turn conversations\r\n\r\nclass ChatResponse(BaseModel):\r\n    response: str\r\n    sources: List[Dict]\r\n    query: str\r\n    model_used: str\r\n\r\n@router.post("/chat", response_model=ChatResponse)\r\nasync def chat(\r\n    request: ChatRequest,\r\n    current_user: UserRead = Depends(get_current_user),\r\n    session: AsyncSession = Depends(get_async_session)\r\n):\r\n    """\r\n    Main chat endpoint for the RAG system\r\n    """\r\n    try:\r\n        # Create user background context\r\n        user_background = {\r\n            "software_background": current_user.software_background,\r\n            "hardware_background": current_user.hardware_background\r\n        }\r\n        \r\n        # Generate response using RAG service\r\n        result = await rag_service.generate_response(\r\n            query=request.message,\r\n            user_background=user_background,\r\n            user_id=current_user.id,\r\n            selected_text=request.selected_text\r\n        )\r\n        \r\n        return ChatResponse(**result)\r\n    \r\n    except Exception as e:\r\n        logging.error(f"Chat error: {e}")\r\n        raise HTTPException(status_code=500, detail="Internal server error during chat")\r\n\r\nclass DocumentSelectionRequest(BaseModel):\r\n    query: str\r\n    document_content: str  # The text content selected by the user\r\n\r\n@router.post("/chat_with_selection")\r\nasync def chat_with_selection(\r\n    request: DocumentSelectionRequest,\r\n    current_user: UserRead = Depends(get_current_user),\r\n    session: AsyncSession = Depends(get_async_session)\r\n):\r\n    """\r\n    Chat endpoint that answers only based on the provided document content\r\n    """\r\n    try:\r\n        # Create user background context\r\n        user_background = {\r\n            "software_background": current_user.software_background,\r\n            "hardware_background": current_user.hardware_background\r\n        }\r\n        \r\n        # Generate response using only the provided content\r\n        result = await rag_service.generate_response(\r\n            query=request.query,\r\n            user_background=user_background,\r\n            user_id=current_user.id,\r\n            selected_text=request.document_content\r\n        )\r\n        \r\n        return ChatResponse(**result)\r\n    \r\n    except Exception as e:\r\n        logging.error(f"Chat with selection error: {e}")\r\n        raise HTTPException(status_code=500, detail="Internal server error during chat with selection")\r\n\r\nclass MultiTurnChatRequest(BaseModel):\r\n    message: str\r\n    session_id: Optional[str] = None\r\n    selected_text: Optional[str] = None\r\n\r\n@router.post("/chat_conversation")\r\nasync def chat_conversation(\r\n    request: MultiTurnChatRequest,\r\n    current_user: UserRead = Depends(get_current_user),\r\n    session: AsyncSession = Depends(get_async_session)\r\n):\r\n    """\r\n    Chat endpoint with conversation history management\r\n    """\r\n    try:\r\n        # Create user background context\r\n        user_background = {\r\n            "software_background": current_user.software_background,\r\n            "hardware_background": current_user.hardware_background\r\n        }\r\n        \r\n        # For now, we\'ll implement a simple conversation without full history\r\n        # In a real implementation, you\'d retrieve conversation history from DB\r\n        result = await rag_service.generate_response(\r\n            query=request.message,\r\n            user_background=user_background,\r\n            user_id=current_user.id,\r\n            selected_text=request.selected_text\r\n        )\r\n        \r\n        # In a full implementation, you would:\r\n        # 1. Generate or retrieve session_id\r\n        # 2. Retrieve conversation history from DB\r\n        # 3. Include history in the prompt to the LLM\r\n        # 4. Save the interaction to DB for future context\r\n        \r\n        return ChatResponse(**result)\r\n    \r\n    except Exception as e:\r\n        logging.error(f"Multi-turn chat error: {e}")\r\n        raise HTTPException(status_code=500, detail="Internal server error during conversation")\n'})}),"\n",(0,s.jsx)(r.h2,{id:"api-dependencies-and-security",children:"API Dependencies and Security"}),"\n",(0,s.jsx)(r.p,{children:"Let's implement dependencies for user authentication and rate limiting:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'# backend/app/api/deps.py\r\nfrom fastapi import Depends, HTTPException, status\r\nfrom fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\r\nfrom sqlalchemy.ext.asyncio import AsyncSession\r\nfrom app.database import get_async_session\r\nfrom app.models.user import User\r\nfrom app.schemas.user import UserRead\r\nfrom better_auth.client import Client\r\nfrom better_auth.security import verify_token\r\nimport logging\r\n\r\nsecurity = HTTPBearer()\r\n\r\nasync def get_current_user(\r\n    credentials: HTTPAuthorizationCredentials = Depends(security),\r\n    db_session: AsyncSession = Depends(get_async_session)\r\n) -> UserRead:\r\n    """\r\n    Get the current authenticated user from the token\r\n    """\r\n    try:\r\n        token = credentials.credentials\r\n        \r\n        # Verify the token using Better Auth\r\n        user_info = await verify_token(token)\r\n        \r\n        if not user_info:\r\n            raise HTTPException(\r\n                status_code=status.HTTP_401_UNAUTHORIZED,\r\n                detail="Invalid authentication credentials",\r\n                headers={"WWW-Authenticate": "Bearer"},\r\n            )\r\n        \r\n        # Retrieve user from database\r\n        user = await db_session.get(User, user_info.get("id"))\r\n        if not user or not user.is_active:\r\n            raise HTTPException(\r\n                status_code=status.HTTP_401_UNAUTHORIZED,\r\n                detail="User account is inactive",\r\n                headers={"WWW-Authenticate": "Bearer"},\r\n            )\r\n        \r\n        return UserRead.from_orm(user)\r\n    \r\n    except Exception as e:\r\n        logging.error(f"Error getting current user: {e}")\r\n        raise HTTPException(\r\n            status_code=status.HTTP_401_UNAUTHORIZED,\r\n            detail="Could not validate credentials",\r\n            headers={"WWW-Authenticate": "Bearer"},\r\n        )\r\n\r\n# Rate limiting implementation (simplified)\r\nfrom functools import wraps\r\nimport time\r\nfrom collections import defaultdict\r\n\r\n# Simple in-memory rate limiter (use Redis in production)\r\nuser_requests = defaultdict(list)\r\n\r\ndef rate_limit(max_requests: int, window_seconds: int):\r\n    """\r\n    Rate limiting decorator\r\n    """\r\n    def decorator(func):\r\n        @wraps(func)\r\n        async def wrapper(*args, **kwargs):\r\n            user_id = kwargs.get(\'current_user\').id if \'current_user\' in kwargs else \'anonymous\'\r\n            \r\n            now = time.time()\r\n            \r\n            # Clean old requests outside the window\r\n            user_requests[user_id] = [\r\n                req_time for req_time in user_requests[user_id] \r\n                if now - req_time < window_seconds\r\n            ]\r\n            \r\n            # Check if user has exceeded the limit\r\n            if len(user_requests[user_id]) >= max_requests:\r\n                raise HTTPException(\r\n                    status_code=status.HTTP_429_TOO_MANY_REQUESTS,\r\n                    detail="Rate limit exceeded"\r\n                )\r\n            \r\n            # Add current request\r\n            user_requests[user_id].append(now)\r\n            \r\n            return await func(*args, **kwargs)\r\n        return wrapper\r\n    return decorator\n'})}),"\n",(0,s.jsx)(r.h2,{id:"configuration-management",children:"Configuration Management"}),"\n",(0,s.jsx)(r.p,{children:"Let's complete the configuration file:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'# backend/app/config.py\r\nfrom pydantic_settings import SettingsConfigDict, BaseSettings\r\nfrom typing import List, Optional\r\n\r\nclass Settings(BaseSettings):\r\n    model_config = SettingsConfigDict(env_file=".env", case_sensitive=True)\r\n    \r\n    # API Settings\r\n    API_V1_STR: str = "/api/v1"\r\n    PROJECT_NAME: str = "RAG Chatbot API"\r\n    \r\n    # Authentication\r\n    AUTH_SECRET: str\r\n    ALLOWED_ORIGINS: List[str] = ["http://localhost:3000", "http://localhost:8000"]\r\n    \r\n    # Database\r\n    NEON_DATABASE_URL: str\r\n    \r\n    # Vector Database\r\n    QDRANT_URL: str\r\n    QDRANT_API_KEY: str\r\n    \r\n    # LLM Settings\r\n    OPENAI_API_KEY: str\r\n    GROK_API_KEY: Optional[str] = None  # Alternative LLM provider\r\n    \r\n    # Application\r\n    DEBUG: bool = False\r\n\r\nsettings = Settings()\n'})}),"\n",(0,s.jsx)(r.h2,{id:"database-models-integration",children:"Database Models Integration"}),"\n",(0,s.jsx)(r.p,{children:"Finally, let's make sure our user model is properly connected:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'# backend/app/models/user.py (updated)\r\nfrom sqlmodel import SQLModel, Field\r\nfrom datetime import datetime\r\nfrom typing import Optional\r\nfrom pydantic import BaseModel\r\n\r\nclass UserBase(BaseModel):\r\n    email: str\r\n    name: Optional[str] = None\r\n\r\nclass User(UserBase):\r\n    id: Optional[int] = Field(default=None, primary_key=True)\r\n    hashed_password: str\r\n    software_background: str = "beginner"  # beginner, intermediate, advanced, expert\r\n    hardware_background: str = "beginner"  # beginner, intermediate, advanced, expert\r\n    is_active: bool = True\r\n    created_at: datetime = Field(default_factory=datetime.utcnow)\r\n    updated_at: datetime = Field(default_factory=datetime.utcnow)\r\n\r\nclass UserCreate(UserBase):\r\n    password: str\r\n    software_background: str = "beginner"\r\n    hardware_background: str = "beginner"\r\n\r\nclass UserRead(UserBase):\r\n    id: int\r\n    software_background: str\r\n    hardware_background: str\r\n    created_at: datetime\n'})}),"\n",(0,s.jsx)(r.h2,{id:"testing-the-backend",children:"Testing the Backend"}),"\n",(0,s.jsx)(r.p,{children:"To ensure everything works together, let's create a simple test:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-python",children:'# backend/tests/test_rag_backend.py\r\nimport pytest\r\nfrom fastapi.testclient import TestClient\r\nfrom app.main import app\r\nfrom unittest.mock import AsyncMock, patch\r\n\r\nclient = TestClient(app)\r\n\r\n@pytest.mark.asyncio\r\nasync def test_chat_endpoint():\r\n    """Test the chat endpoint"""\r\n    # Mock the RAG service\r\n    with patch(\'app.api.v1.chat.rag_service\') as mock_rag_service:\r\n        mock_rag_service.generate_response = AsyncMock(return_value={\r\n            "response": "This is a test response",\r\n            "sources": [],\r\n            "query": "test query",\r\n            "model_used": "gpt-3.5-turbo"\r\n        })\r\n        \r\n        # Mock authentication\r\n        with patch(\'app.api.deps.get_current_user\') as mock_user:\r\n            mock_user.return_value = type(\'User\', (), {\r\n                \'id\': 1,\r\n                \'software_background\': \'beginner\',\r\n                \'hardware_background\': \'beginner\',\r\n                \'is_active\': True\r\n            })()\r\n            \r\n            response = client.post(\r\n                "/api/v1/chat",\r\n                json={"message": "Hello, how does RAG work?"}\r\n            )\r\n            \r\n            assert response.status_code == 200\r\n            data = response.json()\r\n            assert "response" in data\r\n            assert "sources" in data\n'})}),"\n",(0,s.jsxs)(r.p,{children:["Continue to ",(0,s.jsx)(r.a,{href:"/AI_Book_Hackathon/docs/modules/module-5-rag-chatbot/lesson-5-frontend-ui",children:"Lesson 5: Creating the Professional Frontend UI"})," to build the user interface for our RAG chatbot."]})]})}function l(e={}){const{wrapper:r}={...(0,o.R)(),...e.components};return r?(0,s.jsx)(r,{...e,children:(0,s.jsx)(u,{...e})}):u(e)}}}]);
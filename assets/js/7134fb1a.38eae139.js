"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[7760],{6078:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>p,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"modules/module-5-rag-chatbot/lesson-6-integration-deployment","title":"Lesson 6: Integration and Deployment","description":"Introduction","source":"@site/docs/modules/module-5-rag-chatbot/lesson-6-integration-deployment.md","sourceDirName":"modules/module-5-rag-chatbot","slug":"/modules/module-5-rag-chatbot/lesson-6-integration-deployment","permalink":"/AI_Book_Hackathon/docs/modules/module-5-rag-chatbot/lesson-6-integration-deployment","draft":false,"unlisted":false,"editUrl":"https://github.com/mussawirsohail/AI_Book_Hackathon/docs/modules/module-5-rag-chatbot/lesson-6-integration-deployment.md","tags":[],"version":"current","sidebarPosition":6,"frontMatter":{"sidebar_position":6},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 5: Creating the Professional Frontend UI","permalink":"/AI_Book_Hackathon/docs/modules/module-5-rag-chatbot/lesson-5-frontend-ui"},"next":{"title":"Physical AI & Humanoid Robotics Assistant","permalink":"/AI_Book_Hackathon/docs/modules/module-5-rag-chatbot/lesson-7-physical-ai-assistant"}}');var s=r(4848),o=r(8453);const a={sidebar_position:6},i="Lesson 6: Integration and Deployment",c={},l=[{value:"Introduction",id:"introduction",level:2},{value:"Environment Configuration",id:"environment-configuration",level:2},{value:"Complete Backend with All Dependencies",id:"complete-backend-with-all-dependencies",level:2},{value:"Docker Configuration",id:"docker-configuration",level:2},{value:"Deployment Configuration",id:"deployment-configuration",level:2},{value:"Heroku Deployment",id:"heroku-deployment",level:3},{value:"Vercel Deployment for Frontend",id:"vercel-deployment-for-frontend",level:3},{value:"Production Security Measures",id:"production-security-measures",level:2},{value:"Testing in Production Environment",id:"testing-in-production-environment",level:2},{value:"Monitoring and Analytics",id:"monitoring-and-analytics",level:2},{value:"Documentation and Deployment Scripts",id:"documentation-and-deployment-scripts",level:2},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Final Integration Steps",id:"final-integration-steps",level:2},{value:"Final Testing and Quality Assurance",id:"final-testing-and-quality-assurance",level:2},{value:"Summary and Next Steps",id:"summary-and-next-steps",level:2},{value:"Next Steps for Production:",id:"next-steps-for-production",level:3},{value:"Going Live Checklist:",id:"going-live-checklist",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"lesson-6-integration-and-deployment",children:"Lesson 6: Integration and Deployment"})}),"\n",(0,s.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,s.jsx)(n.p,{children:"In this final lesson, we'll integrate all components of our RAG chatbot system and prepare for deployment. This includes connecting the frontend to the backend, configuring production settings, and setting up deployment infrastructure."}),"\n",(0,s.jsx)(n.h2,{id:"environment-configuration",children:"Environment Configuration"}),"\n",(0,s.jsx)(n.p,{children:"First, let's set up our environment configuration files for both backend and frontend:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# backend/.env\r\n# Database\r\nNEON_DATABASE_URL=postgresql://username:password@ep-xxx.us-east-1.aws.neon.tech/dbname?sslmode=require\r\n\r\n# Vector Database\r\nQDRANT_URL=https://your-cluster-url.qdrant.tech\r\nQDRANT_API_KEY=your-qdrant-api-key\r\n\r\n# LLM API\r\nOPENAI_API_KEY=your-openai-api-key\r\nGROK_API_KEY=your-grok-api-key\r\n\r\n# Authentication\r\nAUTH_SECRET=your-super-secret-auth-key\r\nALLOWED_ORIGINS=["http://localhost:3000", "https://yourdomain.com"]\r\n\r\n# Application\r\nDEBUG=False\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# frontend/.env\r\nREACT_APP_API_URL=https://mussawirsoomro5-physical-ai.hf.space/api/v1\r\nREACT_APP_AUTH_URL=https://mussawirsoomro5-physical-ai.hf.space\n"})}),"\n",(0,s.jsx)(n.h2,{id:"complete-backend-with-all-dependencies",children:"Complete Backend with All Dependencies"}),"\n",(0,s.jsx)(n.p,{children:"Let's create our final backend setup with all necessary files:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# backend/requirements.txt\r\nfastapi==0.104.1\r\nuvicorn[standard]==0.24.0\r\nsqlmodel==0.0.16\r\npydantic==2.5.0\r\npydantic-settings==2.1.0\r\nbetter-auth==0.0.1-beta.17\r\nasyncpg==0.29.0\r\nopenai==1.3.4\r\nqdrant-client==1.9.1\r\npython-multipart==0.0.6\r\ntiktoken==0.5.2\r\nlangchain==0.0.352\r\nPyPDF2==3.0.1\r\npython-dotenv==1.0.0\r\npytest==7.4.3\r\npytest-asyncio==0.21.1\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# backend/app/database.py\r\nfrom sqlmodel import create_engine, Session\r\nfrom sqlmodel.ext.asyncio.session import AsyncSession\r\nfrom sqlalchemy.ext.asyncio import create_async_engine\r\nfrom app.config import settings\r\nfrom typing import AsyncGenerator\r\n\r\n# Create async engine\r\nasync_engine = create_async_engine(\r\n    settings.NEON_DATABASE_URL,\r\n    echo=settings.DEBUG\r\n)\r\n\r\nasync def get_async_session() -> AsyncGenerator[AsyncSession, None]:\r\n    async with AsyncSession(async_engine) as session:\r\n        yield session\n"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# backend/app/main.py\r\nfrom fastapi import FastAPI\r\nfrom fastapi.middleware.cors import CORSMiddleware\r\nfrom app.api.v1.api import api_router\r\nfrom app.config import settings\r\nfrom app.database import async_engine\r\nfrom sqlmodel.ext.asyncio.session import AsyncSession\r\nfrom contextlib import asynccontextmanager\r\nimport logging\r\n\r\nlogging.basicConfig(level=logging.INFO)\r\nlogger = logging.getLogger(__name__)\r\n\r\n@asynccontextmanager\r\nasync def lifespan(app: FastAPI):\r\n    # Initialize resources on startup\r\n    logger.info("Initializing resources...")\r\n    # Initialize vector database connection\r\n    # Initialize LLM clients\r\n    yield\r\n    # Clean up resources on shutdown\r\n    await async_engine.dispose()\r\n    logger.info("Resources cleaned up")\r\n\r\napp = FastAPI(\r\n    title="RAG Chatbot API",\r\n    description="API for Retrieval-Augmented Generation chatbot",\r\n    version="1.0.0",\r\n    openapi_url="/api/v1/openapi.json",\r\n    lifespan=lifespan\r\n)\r\n\r\n# Add CORS middleware\r\napp.add_middleware(\r\n    CORSMiddleware,\r\n    allow_origins=settings.ALLOWED_ORIGINS,\r\n    allow_credentials=True,\r\n    allow_methods=["*"],\r\n    allow_headers=["*"],\r\n    # Expose headers for authentication\r\n    expose_headers=["Access-Control-Allow-Origin"]\r\n)\r\n\r\n# Include API routes\r\napp.include_router(api_router, prefix="/api/v1")\r\n\r\n@app.get("/")\r\ndef read_root():\r\n    return {"message": "RAG Chatbot API is running!"}\r\n\r\n@app.get("/health")\r\ndef health_check():\r\n    return {"status": "healthy"}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"docker-configuration",children:"Docker Configuration"}),"\n",(0,s.jsx)(n.p,{children:"Let's create Docker files for both backend and frontend:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-dockerfile",children:'# backend/Dockerfile\r\nFROM python:3.10-slim\r\n\r\nWORKDIR /app\r\n\r\n# Install system dependencies\r\nRUN apt-get update && apt-get install -y \\\r\n    gcc \\\r\n    && rm -rf /var/lib/apt/lists/*\r\n\r\n# Copy requirements and install dependencies\r\nCOPY requirements.txt .\r\nRUN pip install --no-cache-dir -r requirements.txt\r\n\r\n# Copy the rest of the application\r\nCOPY . .\r\n\r\n# Create non-root user\r\nRUN adduser --disabled-password --gecos \'\' appuser\r\nRUN chown -R appuser:appuser /app\r\nUSER appuser\r\n\r\n# Expose port\r\nEXPOSE 8000\r\n\r\n# Run the application\r\nCMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-dockerfile",children:'# frontend/Dockerfile\r\nFROM node:18-alpine\r\n\r\nWORKDIR /app\r\n\r\n# Copy package files\r\nCOPY package*.json ./\r\nRUN npm ci --only=production\r\n\r\n# Copy source code\r\nCOPY . .\r\n\r\n# Build the application\r\nRUN npm run build\r\n\r\n# Install serve to serve the static files\r\nRUN npm install -g serve\r\n\r\n# Expose port\r\nEXPOSE 3000\r\n\r\n# Start the application\r\nCMD ["serve", "-s", "build", "-l", "3000"]\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# docker-compose.yml\r\nversion: \'3.8\'\r\n\r\nservices:\r\n  backend:\r\n    build:\r\n      context: ./backend\r\n    ports:\r\n      - "8000:8000"\r\n    environment:\r\n      - NEON_DATABASE_URL=${NEON_DATABASE_URL}\r\n      - QDRANT_URL=${QDRANT_URL}\r\n      - QDRANT_API_KEY=${QDRANT_API_KEY}\r\n      - OPENAI_API_KEY=${OPENAI_API_KEY}\r\n      - AUTH_SECRET=${AUTH_SECRET}\r\n      - DEBUG=${DEBUG}\r\n    depends_on:\r\n      - db\r\n    env_file:\r\n      - ./backend/.env\r\n\r\n  frontend:\r\n    build:\r\n      context: ./frontend\r\n    ports:\r\n      - "3000:3000"\r\n    environment:\r\n      - REACT_APP_API_URL=${REACT_APP_API_URL}\r\n    depends_on:\r\n      - backend\r\n\r\nvolumes:\r\n  postgres_data:\n'})}),"\n",(0,s.jsx)(n.h2,{id:"deployment-configuration",children:"Deployment Configuration"}),"\n",(0,s.jsx)(n.p,{children:"Let's create deployment configurations for different platforms:"}),"\n",(0,s.jsx)(n.h3,{id:"heroku-deployment",children:"Heroku Deployment"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'# backend/app.json\r\n{\r\n  "name": "RAG Chatbot Backend",\r\n  "description": "Backend for RAG Chatbot using FastAPI",\r\n  "repository": "https://github.com/your-repo/rag-chatbot",\r\n  "env": {\r\n    "NEON_DATABASE_URL": {\r\n      "description": "Neon Postgres database URL",\r\n      "required": true\r\n    },\r\n    "QDRANT_URL": {\r\n      "description": "Qdrant cloud URL",\r\n      "required": true\r\n    },\r\n    "QDRANT_API_KEY": {\r\n      "description": "Qdrant API key",\r\n      "required": true\r\n    },\r\n    "OPENAI_API_KEY": {\r\n      "description": "OpenAI API key",\r\n      "required": true\r\n    },\r\n    "AUTH_SECRET": {\r\n      "description": "Secret key for authentication",\r\n      "required": true,\r\n      "generator": "secret"\r\n    },\r\n    "DEBUG": {\r\n      "description": "Debug mode",\r\n      "value": "False"\r\n    }\r\n  },\r\n  "formation": {\r\n    "web": {\r\n      "quantity": 1,\r\n      "size": "basic"\r\n    }\r\n  },\r\n  "buildpacks": [\r\n    {\r\n      "url": "heroku/python"\r\n    }\r\n  ]\r\n}\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# backend/Procfile\r\nweb: uvicorn app.main:app --host=0.0.0.0 --port=${PORT:-8000}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"vercel-deployment-for-frontend",children:"Vercel Deployment for Frontend"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-json",children:'// frontend/vercel.json\r\n{\r\n  "version": 2,\r\n  "builds": [\r\n    {\r\n      "src": "package.json",\r\n      "use": "@vercel/static-build",\r\n      "config": {\r\n        "distDir": "build"\r\n      }\r\n    }\r\n  ],\r\n  "routes": [\r\n    {\r\n      "src": "/(.*)",\r\n      "dest": "/index.html"\r\n    }\r\n  ],\r\n  "env": {\r\n    "REACT_APP_API_URL": "https://your-backend-url.com/api/v1"\r\n  }\r\n}\n'})}),"\n",(0,s.jsx)(n.h2,{id:"production-security-measures",children:"Production Security Measures"}),"\n",(0,s.jsx)(n.p,{children:"Implement security best practices for production:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# backend/app/security.py\r\nfrom fastapi import HTTPException, status, Request\r\nfrom fastapi.security import HTTPBearer\r\nfrom better_auth.security import verify_token\r\nimport secrets\r\nimport logging\r\n\r\nsecurity = HTTPBearer()\r\n\r\ndef verify_api_key_header(api_key: str = None):\r\n    """\r\n    Verify API key in header\r\n    """\r\n    expected_api_key = "your-expected-api-key"\r\n    if not api_key or secrets.compare_digest(api_key, expected_api_key):\r\n        raise HTTPException(\r\n            status_code=status.HTTP_401_UNAUTHORIZED,\r\n            detail="Invalid API Key"\r\n        )\r\n\r\nasync def verify_auth_token(token: str):\r\n    """\r\n    Verify authentication token\r\n    """\r\n    try:\r\n        user_info = await verify_token(token)\r\n        if not user_info:\r\n            raise HTTPException(\r\n                status_code=status.HTTP_401_UNAUTHORIZED,\r\n                detail="Invalid authentication credentials"\r\n            )\r\n        return user_info\r\n    except Exception as e:\r\n        logging.error(f"Token verification error: {e}")\r\n        raise HTTPException(\r\n            status_code=status.HTTP_401_UNAUTHORIZED,\r\n            detail="Invalid authentication credentials"\r\n        )\r\n\r\ndef rate_limit(max_requests: int, window_seconds: int):\r\n    """\r\n    Rate limiting middleware\r\n    """\r\n    def decorator(func):\r\n        storage = {}  # In production, use Redis or database\r\n        \r\n        async def wrapper(*args, **kwargs):\r\n            # Implementation for rate limiting\r\n            # Extract client IP or user ID\r\n            # Track requests and enforce limits\r\n            return await func(*args, **kwargs)\r\n        return wrapper\r\n    return decorator\n'})}),"\n",(0,s.jsx)(n.h2,{id:"testing-in-production-environment",children:"Testing in Production Environment"}),"\n",(0,s.jsx)(n.p,{children:"Create comprehensive tests for the integrated system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# backend/tests/integration_test.py\r\nimport pytest\r\nfrom httpx import AsyncClient\r\nfrom app.main import app\r\nfrom app.database import async_engine\r\nfrom sqlmodel.ext.asyncio.session import AsyncSession\r\nfrom sqlmodel import SQLModel\r\nimport asyncio\r\n\r\n@pytest.fixture(scope="session")\r\ndef event_loop():\r\n    loop = asyncio.get_event_loop_policy().new_event_loop()\r\n    yield loop\r\n    loop.close()\r\n\r\n@pytest.fixture(scope="session", autouse=True)\r\nasync def setup_test_db():\r\n    """Create test database tables"""\r\n    async with async_engine.begin() as conn:\r\n        await conn.run_sync(SQLModel.metadata.create_all)\r\n    yield\r\n    async with async_engine.begin() as conn:\r\n        await conn.run_sync(SQLModel.metadata.drop_all)\r\n\r\n@pytest.fixture\r\nasync def async_client():\r\n    async with AsyncClient(app=app, base_url="http://test") as ac:\r\n        yield ac\r\n\r\n@pytest.mark.asyncio\r\nasync def test_chat_endpoint_integration(async_client):\r\n    """Test the complete chat pipeline"""\r\n    # First, we\'d need to authenticate to get a token\r\n    # For this example, we\'ll mock the authentication\r\n    \r\n    # Test chat endpoint\r\n    response = await async_client.post(\r\n        "/api/v1/chat",\r\n        json={"message": "Hello, how does RAG work?"}\r\n    )\r\n    \r\n    assert response.status_code == 200\r\n    data = response.json()\r\n    assert "response" in data\r\n    assert "sources" in data\r\n\r\n@pytest.mark.asyncio\r\nasync def test_document_upload_integration(async_client):\r\n    """Test document upload and retrieval"""\r\n    # Similar test for document upload functionality\r\n    pass\n'})}),"\n",(0,s.jsx)(n.h2,{id:"monitoring-and-analytics",children:"Monitoring and Analytics"}),"\n",(0,s.jsx)(n.p,{children:"Add monitoring capabilities to track usage and performance:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# backend/app/middleware/monitoring.py\r\nfrom fastapi import Request, Response\r\nfrom starlette.middleware.base import BaseHTTPMiddleware\r\nfrom datetime import datetime\r\nimport time\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass MonitoringMiddleware(BaseHTTPMiddleware):\r\n    async def dispatch(self, request: Request, call_next):\r\n        start_time = time.time()\r\n        \r\n        try:\r\n            response: Response = await call_next(request)\r\n        except Exception as e:\r\n            # Log the error\r\n            logger.error(f"Request error: {str(e)}", exc_info=True)\r\n            raise\r\n        finally:\r\n            process_time = time.time() - start_time\r\n            formatted_process_time = f"{process_time:.4f}"\r\n            \r\n            logger.info(\r\n                f"{request.method} {request.url.path} "\r\n                f"{response.status_code} {formatted_process_time}s"\r\n            )\r\n            \r\n            response.headers["X-Process-Time"] = formatted_process_time\r\n            \r\n        return response\r\n\r\n# Add to main app\r\n# app.add_middleware(MonitoringMiddleware)\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# backend/app/services/analytics.py\r\nimport asyncio\r\nfrom typing import Dict, Any\r\nfrom datetime import datetime\r\nfrom app.database import get_async_session\r\nfrom app.models.interaction import Interaction\r\n\r\nclass AnalyticsService:\r\n    def __init__(self):\r\n        pass\r\n    \r\n    async def log_interaction(\r\n        self, \r\n        user_id: str, \r\n        query: str, \r\n        response: str, \r\n        sources: list,\r\n        processing_time: float\r\n    ):\r\n        """Log user interactions for analytics"""\r\n        try:\r\n            # In a real implementation, you\'d store this in a database\r\n            # or send to an analytics service\r\n            interaction = Interaction(\r\n                user_id=user_id,\r\n                query=query,\r\n                response=response,\r\n                sources=sources,\r\n                processing_time=processing_time,\r\n                timestamp=datetime.utcnow()\r\n            )\r\n            \r\n            # Store in database\r\n            async with get_async_session() as session:\r\n                session.add(interaction)\r\n                await session.commit()\r\n                \r\n        except Exception as e:\r\n            # Log error but don\'t fail the main operation\r\n            print(f"Analytics error: {e}")\r\n\r\nanalytics_service = AnalyticsService()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"documentation-and-deployment-scripts",children:"Documentation and Deployment Scripts"}),"\n",(0,s.jsx)(n.p,{children:"Create deployment scripts to simplify the process:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# deploy.sh\r\n#!/bin/bash\r\n\r\necho "Starting deployment of RAG Chatbot..."\r\n\r\n# Build backend\r\necho "Building backend..."\r\ncd backend\r\ndocker build -t rag-chatbot-backend .\r\n\r\n# Build frontend\r\necho "Building frontend..."\r\ncd ../frontend\r\ndocker build -t rag-chatbot-frontend .\r\n\r\n# Deploy with docker-compose\r\necho "Starting services..."\r\ncd ..\r\ndocker-compose up -d\r\n\r\necho "Deployment completed!"\r\necho "Backend available at https://mussawirsoomro5-physical-ai.hf.space"\r\necho "Frontend available at http://localhost:3000"\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# k8s/rag-chatbot.yml\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: rag-chatbot-backend\r\nspec:\r\n  replicas: 2\r\n  selector:\r\n    matchLabels:\r\n      app: rag-chatbot-backend\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: rag-chatbot-backend\r\n    spec:\r\n      containers:\r\n      - name: backend\r\n        image: rag-chatbot-backend:latest\r\n        ports:\r\n        - containerPort: 8000\r\n        env:\r\n        - name: DATABASE_URL\r\n          valueFrom:\r\n            secretKeyRef:\r\n              name: rag-chatbot-secrets\r\n              key: database_url\r\n        - name: QDRANT_URL\r\n          valueFrom:\r\n            secretKeyRef:\r\n              name: rag-chatbot-secrets\r\n              key: qdrant_url\r\n---\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: rag-chatbot-backend-service\r\nspec:\r\n  selector:\r\n    app: rag-chatbot-backend\r\n  ports:\r\n    - protocol: TCP\r\n      port: 80\r\n      targetPort: 8000\r\n  type: LoadBalancer\n"})}),"\n",(0,s.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,s.jsx)(n.p,{children:"Implement caching and optimization techniques:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# backend/app/services/cache.py\r\nimport aioredis\r\nfrom typing import Optional, Any\r\nimport json\r\nimport logging\r\n\r\nlogger = logging.getLogger(__name__)\r\n\r\nclass CacheService:\r\n    def __init__(self, redis_url: str):\r\n        self.redis_url = redis_url\r\n        self.redis = None\r\n    \r\n    async def connect(self):\r\n        """Connect to Redis"""\r\n        self.redis = await aioredis.from_url(\r\n            self.redis_url,\r\n            encoding="utf-8",\r\n            decode_responses=True\r\n        )\r\n    \r\n    async def get(self, key: str) -> Optional[Any]:\r\n        """Get value from cache"""\r\n        try:\r\n            value = await self.redis.get(key)\r\n            if value:\r\n                return json.loads(value)\r\n        except Exception as e:\r\n            logger.error(f"Cache get error: {e}")\r\n        return None\r\n    \r\n    async def set(self, key: str, value: Any, expire: int = 3600) -> bool:\r\n        """Set value in cache"""\r\n        try:\r\n            await self.redis.setex(\r\n                key, \r\n                expire, \r\n                json.dumps(value)\r\n            )\r\n            return True\r\n        except Exception as e:\r\n            logger.error(f"Cache set error: {e}")\r\n            return False\r\n    \r\n    async def delete(self, key: str) -> bool:\r\n        """Delete value from cache"""\r\n        try:\r\n            await self.redis.delete(key)\r\n            return True\r\n        except Exception as e:\r\n            logger.error(f"Cache delete error: {e}")\r\n            return False\r\n\r\n# Initialize cache\r\ncache_service = CacheService("redis://localhost:6379")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"final-integration-steps",children:"Final Integration Steps"}),"\n",(0,s.jsx)(n.p,{children:"Let's update the backend to use the cache service for performance:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# backend/app/services/rag_service.py (updated)\r\nimport asyncio\r\nfrom typing import List, Dict, Any, Optional\r\nfrom app.vector_db import VectorDBManager\r\nfrom app.document_processor import DocumentProcessor\r\nfrom app.config import settings\r\nfrom app.models.user import UserRead\r\nfrom app.services.cache import cache_service\r\nimport openai\r\nimport logging\r\nimport hashlib\r\n\r\nclass RAGService:\r\n    def __init__(self):\r\n        self.vector_db = VectorDBManager(\r\n            url=settings.QDRANT_URL, \r\n            api_key=settings.QDRANT_API_KEY\r\n        )\r\n        self.doc_processor = DocumentProcessor()\r\n        self.openai_client = openai.AsyncOpenAI(api_key=settings.OPENAI_API_KEY)\r\n        \r\n    async def generate_response(\r\n        self, \r\n        query: str, \r\n        user_background: Optional[Dict[str, str]] = None,\r\n        user_id: Optional[str] = None,\r\n        selected_text: Optional[str] = None\r\n    ) -> Dict[str, Any]:\r\n        """\r\n        Generate a response using RAG approach with caching\r\n        """\r\n        try:\r\n            # Create cache key\r\n            cache_key = self._create_cache_key(query, user_background, selected_text)\r\n            \r\n            # Try to get from cache first\r\n            cached_result = await cache_service.get(cache_key)\r\n            if cached_result:\r\n                logging.info(f"Cache hit for query: {query[:50]}...")\r\n                return cached_result\r\n            \r\n            # If not in cache, generate response\r\n            start_time = asyncio.get_event_loop().time()\r\n            \r\n            # 1. If selected text is provided, use only that as context\r\n            context_texts = []\r\n            if selected_text:\r\n                context_texts = [selected_text]\r\n                sources = []\r\n            else:\r\n                # 2. Generate embedding for the query\r\n                query_embedding = await self.doc_processor.embedding_model.aembed_query(query)\r\n                \r\n                # 3. Search for relevant documents in vector database\r\n                search_results = await self.vector_db.search_similar(\r\n                    query_embedding, \r\n                    top_k=5\r\n                )\r\n                \r\n                # Extract the content from search results\r\n                context_texts = [result["content"] for result in search_results]\r\n                sources = search_results\r\n\r\n            # 4. Create personalized context based on user background\r\n            context = self._create_context_with_user_background(\r\n                query, context_texts, user_background\r\n            )\r\n            \r\n            # 5. Generate response using LLM\r\n            response = await self._generate_llm_response(context, query)\r\n            \r\n            # 6. Create response with metadata\r\n            result = {\r\n                "response": response,\r\n                "sources": sources,\r\n                "query": query,\r\n                "model_used": "gpt-3.5-turbo"\r\n            }\r\n            \r\n            # 7. Calculate processing time\r\n            processing_time = asyncio.get_event_loop().time() - start_time\r\n            \r\n            # 8. Cache the result (only for common queries)\r\n            if len(query.strip()) > 10:  # Only cache for non-trivial queries\r\n                await cache_service.set(cache_key, result, expire=3600)  # Cache for 1 hour\r\n            \r\n            # 9. Log the interaction\r\n            await self._log_interaction(query, response, user_id, sources, processing_time)\r\n            \r\n            return result\r\n            \r\n        except Exception as e:\r\n            logging.error(f"Error in RAG generation: {e}")\r\n            raise\r\n    \r\n    def _create_cache_key(self, query: str, user_background: Optional[Dict], selected_text: Optional[str]) -> str:\r\n        """Create a cache key based on query and context"""\r\n        key_content = f"{query}:{selected_text}:{user_background}"\r\n        return f"rag_response:{hashlib.md5(key_content.encode()).hexdigest()}"\r\n    \r\n    def _create_context_with_user_background(\r\n        self, \r\n        query: str, \r\n        context_texts: List[str], \r\n        user_background: Optional[Dict[str, str]]\r\n    ) -> str:\r\n        """Create a context string with user background information for personalization"""\r\n        # Start with background information if available\r\n        background_context = ""\r\n        if user_background:\r\n            software_bg = user_background.get("software_background", "beginner")\r\n            hardware_bg = user_background.get("hardware_background", "beginner")\r\n            \r\n            # Adjust the persona based on user\'s background\r\n            if software_bg == "beginner" or hardware_bg == "beginner":\r\n                background_context = "Explain concepts in simple terms with practical examples. "\r\n            elif software_bg == "expert" or hardware_bg == "expert":\r\n                background_context = "Provide detailed technical explanations with advanced concepts. "\r\n            else:\r\n                background_context = "Provide balanced explanations with appropriate detail. "\r\n        \r\n        # Combine background with the context from documents\r\n        full_context = background_context + "\\n\\n"\r\n        \r\n        if context_texts:\r\n            full_context += "Relevant information from the book:\\n\\n"\r\n            for i, text in enumerate(context_texts):\r\n                full_context += f"Source {i+1}:\\n{text}\\n\\n"\r\n        else:\r\n            full_context += "No relevant information was found in the book. "\r\n            \r\n        return full_context\r\n    \r\n    async def _generate_llm_response(self, context: str, query: str) -> str:\r\n        """Generate a response using the LLM with the provided context"""\r\n        # Create the prompt for the LLM\r\n        prompt = f"""\r\n        You are a helpful assistant for a book. Answer the user\'s question based on the context provided below.\r\n        \r\n        Context: {context}\r\n        \r\n        User\'s question: {query}\r\n        \r\n        Instructions:\r\n        - If the context contains relevant information, use it to answer the question.\r\n        - If the context doesn\'t contain relevant information, state that you don\'t have enough information from the book to answer.\r\n        - Be concise and helpful in your response.\r\n        - Cite sources when possible using the source numbers mentioned in the context.\r\n        """\r\n        \r\n        try:\r\n            response = await self.openai_client.chat.completions.create(\r\n                model="gpt-3.5-turbo",\r\n                messages=[\r\n                    {"role": "system", "content": "You are a helpful assistant that answers questions based on provided context from a book."},\r\n                    {"role": "user", "content": prompt}\r\n                ],\r\n                max_tokens=1000,\r\n                temperature=0.3,\r\n            )\r\n            \r\n            return response.choices[0].message.content\r\n        \r\n        except Exception as e:\r\n            logging.error(f"Error generating LLM response: {e}")\r\n            raise\r\n    \r\n    async def _log_interaction(\r\n        self, \r\n        query: str, \r\n        response: str, \r\n        user_id: Optional[str], \r\n        sources: List[Dict],\r\n        processing_time: float\r\n    ):\r\n        """Log the interaction for analytics and improvement"""\r\n        # This would typically store in a database for analytics\r\n        logging.info(f"Interaction logged - Query: {query[:50]}... | User ID: {user_id} | Time: {processing_time}s")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"final-testing-and-quality-assurance",children:"Final Testing and Quality Assurance"}),"\n",(0,s.jsx)(n.p,{children:"Create a comprehensive test to verify the entire system works:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# backend/test_system.py\r\nimport asyncio\r\nimport pytest\r\nfrom fastapi.testclient import TestClient\r\nfrom app.main import app\r\nfrom unittest.mock import AsyncMock, patch\r\n\r\nclient = TestClient(app)\r\n\r\ndef test_full_chatbot_flow():\r\n    """End-to-end test of the chatbot functionality"""\r\n    # Mock the external services to avoid making real API calls in tests\r\n    with patch(\'app.services.rag_service.RAGService.generate_response\') as mock_generate:\r\n        mock_generate.return_value = {\r\n            "response": "This is a test response based on the book content.",\r\n            "sources": [{"id": "1", "content": "Sample book content..."}],\r\n            "query": "What is RAG?",\r\n            "model_used": "gpt-3.5-turbo"\r\n        }\r\n        \r\n        # Test the chat endpoint\r\n        response = client.post(\r\n            "/api/v1/chat",\r\n            json={"message": "What is RAG?"},\r\n            headers={"Authorization": "Bearer mock-token"}\r\n        )\r\n        \r\n        assert response.status_code == 200\r\n        data = response.json()\r\n        assert data["response"] == "This is a test response based on the book content."\r\n        assert "sources" in data\r\n        assert data["query"] == "What is RAG?"\r\n\r\nif __name__ == "__main__":\r\n    test_full_chatbot_flow()\r\n    print("All tests passed!")\n'})}),"\n",(0,s.jsx)(n.h2,{id:"summary-and-next-steps",children:"Summary and Next Steps"}),"\n",(0,s.jsx)(n.p,{children:"You have successfully implemented a complete RAG chatbot system with:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Authentication System"}),": Using Better Auth with user background information"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vector Database"}),": Using Qdrant Cloud for efficient similarity search"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Backend API"}),": FastAPI implementation with RAG logic"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Frontend UI"}),": Professional React interface with document selection"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Deployment Configuration"}),": Docker and environment configurations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Security Measures"}),": Authentication, rate limiting, and monitoring"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance Optimization"}),": Caching and efficient processing"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"next-steps-for-production",children:"Next Steps for Production:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Scale the Infrastructure"}),": Implement load balancing and auto-scaling"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Enhance Monitoring"}),": Add application performance monitoring (APM)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Improve Security"}),": Add more security headers and implement OAuth providers"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Optimize Costs"}),": Set up usage-based billing if needed"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Enhance Personalization"}),": Use more sophisticated user profiling"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Add Analytics"}),": Implement comprehensive user behavior analytics"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"going-live-checklist",children:"Going Live Checklist:"}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Verify all environment variables are properly set"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Test the complete flow with real book content"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Ensure SSL certificates are installed"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Set up backup procedures for databases"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Configure monitoring and alerting"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Perform load testing"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Review security configurations"]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Set up CI/CD pipeline for automated deployments"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Your RAG chatbot is now ready for deployment and will provide users with an intelligent, personalized way to interact with your book content!"})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>i});var t=r(6540);const s={},o=t.createContext(s);function a(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);
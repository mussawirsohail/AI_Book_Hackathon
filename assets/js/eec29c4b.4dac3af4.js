"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[4692],{4864:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>s,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"modules/module-5-rag-chatbot/lesson-3-vector-database","title":"Lesson 3: Vector Database Setup with Qdrant","description":"Introduction to Vector Databases","source":"@site/docs/modules/module-5-rag-chatbot/lesson-3-vector-database.md","sourceDirName":"modules/module-5-rag-chatbot","slug":"/modules/module-5-rag-chatbot/lesson-3-vector-database","permalink":"/AI_Book_Hackathon/docs/modules/module-5-rag-chatbot/lesson-3-vector-database","draft":false,"unlisted":false,"editUrl":"https://github.com/mussawirsohail/AI_Book_Hackathon/docs/modules/module-5-rag-chatbot/lesson-3-vector-database.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3}}');var o=r(4848),i=r(8453);const s={sidebar_position:3},a="Lesson 3: Vector Database Setup with Qdrant",c={},d=[{value:"Introduction to Vector Databases",id:"introduction-to-vector-databases",level:2},{value:"Why Qdrant?",id:"why-qdrant",level:2},{value:"Setting Up Qdrant Cloud Free Tier",id:"setting-up-qdrant-cloud-free-tier",level:2},{value:"Collection Schema Design",id:"collection-schema-design",level:2},{value:"Document Preprocessing Pipeline",id:"document-preprocessing-pipeline",level:2},{value:"Integration with FastAPI",id:"integration-with-fastapi",level:2},{value:"Performance Optimization",id:"performance-optimization",level:2},{value:"Configuring Qdrant Collection for Performance",id:"configuring-qdrant-collection-for-performance",level:2}];function l(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"lesson-3-vector-database-setup-with-qdrant",children:"Lesson 3: Vector Database Setup with Qdrant"})}),"\n",(0,o.jsx)(n.h2,{id:"introduction-to-vector-databases",children:"Introduction to Vector Databases"}),"\n",(0,o.jsx)(n.p,{children:"Vector databases store and retrieve data based on similarity rather than exact matches. In a RAG system, the book content is converted into vector embeddings, which allow the system to find the most relevant text passages based on the user's query."}),"\n",(0,o.jsx)(n.h2,{id:"why-qdrant",children:"Why Qdrant?"}),"\n",(0,o.jsx)(n.p,{children:"Qdrant is an open-source vector database that offers:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"High-performance similarity search"}),"\n",(0,o.jsx)(n.li,{children:"Support for various distance metrics (Cosine, Euclidean, Dot Product)"}),"\n",(0,o.jsx)(n.li,{children:"Filtering capabilities"}),"\n",(0,o.jsx)(n.li,{children:"API-first design with SDKs in multiple languages"}),"\n",(0,o.jsx)(n.li,{children:"Support for both local and cloud deployment"}),"\n",(0,o.jsx)(n.li,{children:"Integration with popular embedding models"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"setting-up-qdrant-cloud-free-tier",children:"Setting Up Qdrant Cloud Free Tier"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:["Visit ",(0,o.jsx)(n.a,{href:"https://qdrant.tech",children:"qdrant.tech"})," and create an account"]}),"\n",(0,o.jsx)(n.li,{children:"Create a new collection for your book content"}),"\n",(0,o.jsx)(n.li,{children:"Note your API key and cluster URL for later use"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"collection-schema-design",children:"Collection Schema Design"}),"\n",(0,o.jsx)(n.p,{children:"For our RAG chatbot, we'll create a collection schema that stores both the text content and metadata:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# backend/app/vector_db.py\r\nfrom qdrant_client import QdrantClient\r\nfrom qdrant_client.http import models\r\nfrom qdrant_client.http.models import PointStruct\r\nimport uuid\r\nfrom typing import List, Dict, Any\r\nimport logging\r\n\r\nclass VectorDBManager:\r\n    def __init__(self, url: str, api_key: str):\r\n        self.client = QdrantClient(\r\n            url=url,\r\n            api_key=api_key,\r\n            prefer_grpc=True  # Use gRPC for better performance if available\r\n        )\r\n        self.collection_name = "book_content"\r\n        self.vector_size = 1536  # Size for OpenAI embeddings; adjust for other models\r\n        \r\n    async def create_collection(self):\r\n        """Create a collection in Qdrant for storing book content"""\r\n        try:\r\n            # Check if collection already exists\r\n            collections = await self.client.get_collections()\r\n            collection_names = [c.name for c in collections.collections]\r\n            \r\n            if self.collection_name not in collection_names:\r\n                await self.client.create_collection(\r\n                    collection_name=self.collection_name,\r\n                    vectors_config=models.VectorParams(\r\n                        size=self.vector_size,\r\n                        distance=models.Distance.COSINE\r\n                    )\r\n                )\r\n                \r\n                # Create payload index for metadata\r\n                await self.client.create_payload_index(\r\n                    collection_name=self.collection_name,\r\n                    field_name="document_id",\r\n                    field_schema=models.PayloadSchemaType.KEYWORD\r\n                )\r\n                \r\n                await self.client.create_payload_index(\r\n                    collection_name=self.collection_name,\r\n                    field_name="page_number",\r\n                    field_schema=models.PayloadSchemaType.INTEGER\r\n                )\r\n                \r\n                logging.info(f"Created Qdrant collection: {self.collection_name}")\r\n            else:\r\n                logging.info(f"Collection already exists: {self.collection_name}")\r\n                \r\n        except Exception as e:\r\n            logging.error(f"Error creating collection: {e}")\r\n            raise\r\n\r\n    async def add_documents(self, documents: List[Dict[str, Any]]):\r\n        """Add documents to the Qdrant collection"""\r\n        points = []\r\n        \r\n        for doc in documents:\r\n            # Create a unique ID for each document segment\r\n            point_id = str(uuid.uuid4())\r\n            \r\n            # Each document should have: id, content, embedding, metadata\r\n            point = PointStruct(\r\n                id=point_id,\r\n                vector=doc[\'embedding\'],\r\n                payload={\r\n                    "content": doc[\'content\'],\r\n                    "document_id": doc.get(\'document_id\', \'\'),\r\n                    "page_number": doc.get(\'page_number\', 0),\r\n                    "section_title": doc.get(\'section_title\', \'\'),\r\n                    "source_url": doc.get(\'source_url\', \'\'),\r\n                    "metadata": doc.get(\'metadata\', {})\r\n                }\r\n            )\r\n            \r\n            points.append(point)\r\n        \r\n        # Upload in batches for efficiency\r\n        batch_size = 100\r\n        for i in range(0, len(points), batch_size):\r\n            batch = points[i:i+batch_size]\r\n            await self.client.upsert(\r\n                collection_name=self.collection_name,\r\n                points=batch\r\n            )\r\n        \r\n        logging.info(f"Uploaded {len(points)} documents to Qdrant")\r\n\r\n    async def search_similar(self, query_embedding: List[float], top_k: int = 5, filters: Dict = None) -> List[Dict[str, Any]]:\r\n        """Search for similar documents based on embedding"""\r\n        try:\r\n            # Build filter conditions if provided\r\n            qdrant_filter = None\r\n            if filters:\r\n                conditions = []\r\n                for key, value in filters.items():\r\n                    conditions.append(\r\n                        models.FieldCondition(\r\n                            key=key,\r\n                            match=models.MatchValue(value=value)\r\n                        )\r\n                    )\r\n                \r\n                if conditions:\r\n                    qdrant_filter = models.Filter(must=conditions)\r\n            \r\n            # Perform search\r\n            search_results = await self.client.search(\r\n                collection_name=self.collection_name,\r\n                query_vector=query_embedding,\r\n                limit=top_k,\r\n                with_payload=True,\r\n                score_threshold=0.3,  # Filter out low-similarity matches\r\n                query_filter=qdrant_filter\r\n            )\r\n            \r\n            # Format results\r\n            results = []\r\n            for result in search_results:\r\n                results.append({\r\n                    "id": result.id,\r\n                    "content": result.payload["content"],\r\n                    "document_id": result.payload.get("document_id", ""),\r\n                    "page_number": result.payload.get("page_number", 0),\r\n                    "section_title": result.payload.get("section_title", ""),\r\n                    "score": result.score  # Similarity score\r\n                })\r\n            \r\n            return results\r\n            \r\n        except Exception as e:\r\n            logging.error(f"Error searching Qdrant: {e}")\r\n            raise\n'})}),"\n",(0,o.jsx)(n.h2,{id:"document-preprocessing-pipeline",children:"Document Preprocessing Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"Before storing documents in the vector database, we need to preprocess them:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# backend/app/document_processor.py\r\nimport asyncio\r\nfrom typing import List, Dict, Any\r\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\r\nfrom langchain.embeddings.openai import OpenAIEmbeddings\r\nimport tiktoken\r\nimport logging\r\n\r\nclass DocumentProcessor:\r\n    def __init__(self, embedding_model_name: str = "text-embedding-ada-002"):\r\n        self.text_splitter = RecursiveCharacterTextSplitter(\r\n            chunk_size=1000,\r\n            chunk_overlap=200,\r\n            length_function=self._get_token_count,\r\n            separators=["\\n\\n", "\\n", " ", ""]\r\n        )\r\n        self.embedding_model = OpenAIEmbeddings(model=embedding_model_name)\r\n        \r\n    def _get_token_count(self, text: str) -> int:\r\n        """Get token count using tiktoken"""\r\n        encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")\r\n        return len(encoding.encode(text))\r\n    \r\n    async def preprocess_document(self, content: str, metadata: Dict[str, Any] = None) -> List[Dict[str, Any]]:\r\n        """Preprocess and split documents into chunks for embedding"""\r\n        if metadata is None:\r\n            metadata = {}\r\n            \r\n        # Split the document into chunks\r\n        chunks = self.text_splitter.split_text(content)\r\n        \r\n        # Create documents with metadata\r\n        documents = []\r\n        for i, chunk in enumerate(chunks):\r\n            doc = {\r\n                "content": chunk,\r\n                "document_id": metadata.get("document_id", f"doc_{hash(content[:50])}"),\r\n                "page_number": metadata.get("page_number", i),\r\n                "section_title": metadata.get("section_title", ""),\r\n                "source_url": metadata.get("source_url", ""),\r\n                "metadata": metadata\r\n            }\r\n            documents.append(doc)\r\n        \r\n        return documents\r\n    \r\n    async def generate_embeddings(self, documents: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\r\n        """Generate embeddings for documents in batches"""\r\n        # Extract content for embedding\r\n        contents = [doc["content"] for doc in documents]\r\n        \r\n        # Generate embeddings in batches to respect API limits\r\n        batch_size = 100\r\n        all_embeddings = []\r\n        \r\n        for i in range(0, len(contents), batch_size):\r\n            batch = contents[i:i+batch_size]\r\n            batch_embeddings = await self.embedding_model.aembed_documents(batch)\r\n            \r\n            # Combine documents with their embeddings\r\n            for j, embedding in enumerate(batch_embeddings):\r\n                doc_with_embedding = {**documents[i+j], "embedding": embedding}\r\n                all_embeddings.append(doc_with_embedding)\r\n        \r\n        return all_embeddings\n'})}),"\n",(0,o.jsx)(n.h2,{id:"integration-with-fastapi",children:"Integration with FastAPI"}),"\n",(0,o.jsx)(n.p,{children:"We'll integrate the vector database functionality into our FastAPI application:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# backend/app/api/v1/documents.py\r\nfrom fastapi import APIRouter, Depends, HTTPException, UploadFile, File\r\nfrom sqlalchemy.ext.asyncio import AsyncSession\r\nfrom app.database import get_async_session\r\nfrom app.models.user import UserRead\r\nfrom app.api.deps import get_current_user\r\nfrom app.vector_db import VectorDBManager\r\nfrom app.document_processor import DocumentProcessor\r\nfrom app.config import settings\r\nimport tempfile\r\nimport PyPDF2\r\nimport logging\r\n\r\nrouter = APIRouter()\r\n\r\n@router.post("/upload_document")\r\nasync def upload_document(\r\n    file: UploadFile = File(...),\r\n    session: AsyncSession = Depends(get_async_session),\r\n    current_user: UserRead = Depends(get_current_user)\r\n):\r\n    """Upload and process a document for the RAG system"""\r\n    try:\r\n        # Validate file type\r\n        if not file.content_type.startswith("text/") and file.content_type != "application/pdf":\r\n            raise HTTPException(status_code=400, detail="Only text files and PDFs are supported")\r\n        \r\n        # Read file content\r\n        content = await file.read()\r\n        \r\n        # Process based on file type\r\n        if file.content_type == "application/pdf":\r\n            content = extract_text_from_pdf(content)\r\n        else:\r\n            content = content.decode("utf-8")\r\n        \r\n        # Initialize vector DB and document processor\r\n        vector_db = VectorDBManager(url=settings.QDRANT_URL, api_key=settings.QDRANT_API_KEY)\r\n        doc_processor = DocumentProcessor()\r\n        \r\n        # Preprocess the document\r\n        metadata = {\r\n            "document_id": f"doc_{current_user.id}_{file.filename}",\r\n            "source_url": f"/documents/{file.filename}",\r\n            "uploaded_by": current_user.id\r\n        }\r\n        \r\n        processed_docs = await doc_processor.preprocess_document(content, metadata)\r\n        \r\n        # Generate embeddings\r\n        docs_with_embeddings = await doc_processor.generate_embeddings(processed_docs)\r\n        \r\n        # Add to vector database\r\n        await vector_db.add_documents(docs_with_embeddings)\r\n        \r\n        return {\r\n            "message": f"Successfully uploaded and processed {len(docs_with_embeddings)} document chunks",\r\n            "chunks_count": len(docs_with_embeddings)\r\n        }\r\n        \r\n    except Exception as e:\r\n        logging.error(f"Document upload error: {e}")\r\n        raise HTTPException(status_code=500, detail="Internal server error during document processing")\r\n\r\ndef extract_text_from_pdf(pdf_content: bytes) -> str:\r\n    """Extract text from PDF content"""\r\n    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as temp_pdf:\r\n        temp_pdf.write(pdf_content)\r\n        temp_pdf.flush()\r\n        \r\n        with open(temp_pdf.name, \'rb\') as pdf_file:\r\n            reader = PyPDF2.PdfReader(pdf_file)\r\n            text = ""\r\n            for page in reader.pages:\r\n                text += page.extract_text() + "\\n"\r\n    \r\n    return text\r\n\r\n@router.get("/search")\r\nasync def search_documents(\r\n    query: str,\r\n    top_k: int = 5,\r\n    session: AsyncSession = Depends(get_async_session),\r\n    current_user: UserRead = Depends(get_current_user)\r\n):\r\n    """Search for relevant documents based on the query"""\r\n    try:\r\n        # Initialize vector DB\r\n        vector_db = VectorDBManager(url=settings.QDRANT_URL, api_key=settings.QDRANT_API_KEY)\r\n        \r\n        # Generate embedding for query\r\n        doc_processor = DocumentProcessor()\r\n        query_embedding = await doc_processor.embedding_model.aembed_query(query)\r\n        \r\n        # Search in vector database\r\n        results = await vector_db.search_similar(query_embedding, top_k=top_k)\r\n        \r\n        return {"results": results}\r\n        \r\n    except Exception as e:\r\n        logging.error(f"Document search error: {e}")\r\n        raise HTTPException(status_code=500, detail="Internal server error during search")\n'})}),"\n",(0,o.jsx)(n.h2,{id:"performance-optimization",children:"Performance Optimization"}),"\n",(0,o.jsx)(n.p,{children:"To ensure optimal performance of our vector database:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Proper Indexing"}),": Use HNSW (Hierarchical Navigable Small World) index for faster similarity search"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Batch Operations"}),": Upload documents in batches to reduce API overhead"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Score Thresholding"}),": Filter results to only include highly relevant matches"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Caching"}),": Cache frequently accessed embeddings to reduce API calls"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"configuring-qdrant-collection-for-performance",children:"Configuring Qdrant Collection for Performance"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# backend/app/vector_db_config.py\r\nfrom qdrant_client import QdrantClient\r\nfrom qdrant_client.http import models\r\nfrom app.config import settings\r\n\r\ndef configure_qdrant_collection():\r\n    """Configure the Qdrant collection with optimized settings"""\r\n    client = QdrantClient(\r\n        url=settings.QDRANT_URL,\r\n        api_key=settings.QDRANT_API_KEY\r\n    )\r\n    \r\n    # Create collection with optimized settings\r\n    client.create_collection(\r\n        collection_name="book_content",\r\n        vectors_config=models.VectorParams(\r\n            size=1536,  # Size for OpenAI embeddings\r\n            distance=models.Distance.COSINE\r\n        ),\r\n        # Enable indexing for faster search\r\n        hnsw_config=models.HnswConfigDiff(\r\n            m=16,  # Size of every main index graph\r\n            ef_construct=100,  # Number of neighbours to consider during construction\r\n            full_scan_threshold=10000  # Use plain index if less elements\r\n        ),\r\n        # Enable quantization to reduce memory usage\r\n        quantization_config=models.ScalarQuantization(\r\n            type=models.QuantizationType.INT8,\r\n            quantile=0.99\r\n        )\r\n    )\r\n    \r\n    # Create payload indexes for metadata filtering\r\n    client.create_payload_index(\r\n        collection_name="book_content",\r\n        field_name="document_id",\r\n        field_schema=models.PayloadSchemaType.KEYWORD\r\n    )\r\n    \r\n    client.create_payload_index(\r\n        collection_name="book_content",\r\n        field_name="section_title",\r\n        field_schema=models.PayloadSchemaType.TEXT\r\n    )\r\n\r\nif __name__ == "__main__":\r\n    configure_qdrant_collection()\n'})}),"\n",(0,o.jsxs)(n.p,{children:["Continue to ",(0,o.jsx)(n.a,{href:"/AI_Book_Hackathon/docs/modules/module-5-rag-chatbot/lesson-4-rag-backend",children:"Lesson 4: Building the RAG Backend with FastAPI"})," to implement the complete RAG processing pipeline."]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(l,{...e})}):l(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var t=r(6540);const o={},i=t.createContext(o);function s(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),t.createElement(i.Provider,{value:n},e.children)}}}]);
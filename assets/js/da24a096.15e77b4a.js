"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[7774],{67:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"modules/module-3-ai-brain/lesson-2-isaac-sim-synthetic-data","title":"Lesson 2: Isaac Sim: Photorealistic Simulation and Synthetic Data Generation","description":"Isaac Sim Overview","source":"@site/docs/modules/module-3-ai-brain/lesson-2-isaac-sim-synthetic-data.md","sourceDirName":"modules/module-3-ai-brain","slug":"/modules/module-3-ai-brain/lesson-2-isaac-sim-synthetic-data","permalink":"/AI_Book_Hackathon/docs/modules/module-3-ai-brain/lesson-2-isaac-sim-synthetic-data","draft":false,"unlisted":false,"editUrl":"https://github.com/mussawirsohail/AI_Book_Hackathon/docs/modules/module-3-ai-brain/lesson-2-isaac-sim-synthetic-data.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Lesson 1: Introduction to NVIDIA Isaac and AI-Robot Brains","permalink":"/AI_Book_Hackathon/docs/modules/module-3-ai-brain/lesson-1-introduction-to-isaac"},"next":{"title":"Lesson 3: Isaac ROS and Nav2: Path Planning for Bipedal Humanoid Movement","permalink":"/AI_Book_Hackathon/docs/modules/module-3-ai-brain/lesson-3-isaac-ros-nav2"}}');var t=r(4848),i=r(8453);const s={sidebar_position:2},o="Lesson 2: Isaac Sim: Photorealistic Simulation and Synthetic Data Generation",l={},d=[{value:"Isaac Sim Overview",id:"isaac-sim-overview",level:2},{value:"Photorealistic Rendering in Isaac Sim",id:"photorealistic-rendering-in-isaac-sim",level:2},{value:"Creating Photorealistic Environments",id:"creating-photorealistic-environments",level:3},{value:"Synthetic Data Generation",id:"synthetic-data-generation",level:2},{value:"Configuring Synthetic Data Pipelines",id:"configuring-synthetic-data-pipelines",level:3},{value:"Generating RGB, Depth, and Semantic Segmentation Data",id:"generating-rgb-depth-and-semantic-segmentation-data",level:3},{value:"Variance Generation for Dataset Diversity",id:"variance-generation-for-dataset-diversity",level:2},{value:"Isaac ROS Accelerated VSLAM",id:"isaac-ros-accelerated-vslam",level:2},{value:"Applications of Synthetic Data",id:"applications-of-synthetic-data",level:2},{value:"Best Practices",id:"best-practices",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"lesson-2-isaac-sim-photorealistic-simulation-and-synthetic-data-generation",children:"Lesson 2: Isaac Sim: Photorealistic Simulation and Synthetic Data Generation"})}),"\n",(0,t.jsx)(n.h2,{id:"isaac-sim-overview",children:"Isaac Sim Overview"}),"\n",(0,t.jsx)(n.p,{children:"Isaac Sim is a reference application for robotics simulation based on NVIDIA Omniverse. It provides:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"High-fidelity physics simulation"}),"\n",(0,t.jsx)(n.li,{children:"Photorealistic rendering using NVIDIA RTX technology"}),"\n",(0,t.jsx)(n.li,{children:"Synthetic data generation tools"}),"\n",(0,t.jsx)(n.li,{children:"ROS 2 bridge for integration with robotics frameworks"}),"\n",(0,t.jsx)(n.li,{children:"Reinforcement learning environment support"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"photorealistic-rendering-in-isaac-sim",children:"Photorealistic Rendering in Isaac Sim"}),"\n",(0,t.jsx)(n.p,{children:"Isaac Sim leverages NVIDIA's RTX technology to create photorealistic environments and sensor data. This is essential for generating synthetic data that can be used to train AI models that perform well in the real world."}),"\n",(0,t.jsx)(n.h3,{id:"creating-photorealistic-environments",children:"Creating Photorealistic Environments"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import omni\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\r\nfrom omni.isaac.core.utils.nucleus import get_assets_root_path\r\nfrom omni.isaac.core.utils.prims import get_prim_at_path\r\nfrom pxr import Gf, Sdf, UsdLux, UsdGeom\r\nimport numpy as np\r\n\r\n# Create world and stage\r\nworld = World(stage_units_in_meters=1.0)\r\n\r\n# Access the stage for direct USD manipulation\r\nstage = world.stage\r\n\r\n# Add a dome light for realistic environment lighting\r\ndome_light = UsdLux.DomeLight.Define(stage, Sdf.Path("/World/DomeLight"))\r\ndome_light.CreateIntensityAttr(1000)\r\ndome_light.CreateTextureFileAttr("omniverse://localhost/NVIDIA/Assets/Skies/Indoor/photostudio_01_4k.hdr")\r\n\r\n# Add a physics ground plane\r\nground_plane = world.scene.add_ground_plane("ground", \r\n                                            static_friction=0.7,\r\n                                            dynamic_friction=0.5,\r\n                                            restitution=0.8)\r\n\r\n# Add objects with realistic materials\r\nassets_root_path = get_assets_root_path()\r\nif assets_root_path:\r\n    # Add a realistic textured object\r\n    add_reference_to_stage(\r\n        usd_path=assets_root_path + "/Isaac/Props/KIT/ground_plane.usd",\r\n        prim_path="/World/ground_plane"\r\n    )\n'})}),"\n",(0,t.jsx)(n.h2,{id:"synthetic-data-generation",children:"Synthetic Data Generation"}),"\n",(0,t.jsx)(n.p,{children:"Synthetic data generation is crucial for training AI models when real-world data is scarce, expensive to collect, or dangerous to obtain."}),"\n",(0,t.jsx)(n.h3,{id:"configuring-synthetic-data-pipelines",children:"Configuring Synthetic Data Pipelines"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Example: Setting up semantic segmentation annotation in Isaac Sim\r\nimport omni\r\nfrom omni.isaac.synthetic_utils import SyntheticDataHelper\r\nfrom omni.isaac.core import World\r\nfrom omni.isaac.core.utils import viewports\r\nimport cv2\r\nimport numpy as np\r\n\r\ndef setup_synthetic_data_pipeline():\r\n    # Create world\r\n    world = World(stage_units_in_meters=1.0)\r\n    \r\n    # Set up viewport for rendering\r\n    viewport = viewports.get_viewport_from_window_name("Viewport")\r\n    viewport.set_active_camera("/World/Robot/base_link")\r\n    \r\n    # Enable synthetic data types\r\n    omni.kit.commands.execute("SyntheticDataCreateAnnotatedImageSensor",\r\n                              sensor_prim_path="/World/AnnotatedCamera",\r\n                              annotation_types=["bbox", "semantic_segmentation"])\r\n    \r\n    return world\n'})}),"\n",(0,t.jsx)(n.h3,{id:"generating-rgb-depth-and-semantic-segmentation-data",children:"Generating RGB, Depth, and Semantic Segmentation Data"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'from omni.isaac.synthetic_utils import plot\r\nimport carb\r\nimport cv2\r\nimport numpy as np\r\n\r\nclass SyntheticDataGenerator:\r\n    def __init__(self, world, robot):\r\n        self.world = world\r\n        self.robot = robot\r\n        self.viewport = None\r\n        self.dataset_path = "synthetic_dataset"\r\n        \r\n        # Initialize synthetic data helper\r\n        self.sd_helper = SyntheticDataHelper()\r\n        \r\n    def capture_data_frame(self, frame_id):\r\n        # Step simulation to get new data\r\n        self.world.step(render=True)\r\n        \r\n        # Get different types of sensor data\r\n        rgb_data = self.get_rgb_image()\r\n        depth_data = self.get_depth_image()\r\n        segmentation_data = self.get_segmentation()\r\n        \r\n        # Save data to files\r\n        self.save_image(rgb_data, f"{self.dataset_path}/rgb/frame_{frame_id:06d}.png")\r\n        self.save_image(depth_data, f"{self.dataset_path}/depth/frame_{frame_id:06d}.png")\r\n        self.save_image(segmentation_data, f"{self.dataset_path}/segmentation/frame_{frame_id:06d}.png")\r\n        \r\n        return {\r\n            \'rgb\': rgb_data,\r\n            \'depth\': depth_data,\r\n            \'segmentation\': segmentation_data\r\n        }\r\n    \r\n    def get_rgb_image(self):\r\n        # Code to extract RGB image from viewport\r\n        pass\r\n    \r\n    def get_depth_image(self):\r\n        # Code to extract depth data\r\n        pass\r\n    \r\n    def get_segmentation(self):\r\n        # Code to extract semantic segmentation\r\n        pass\r\n    \r\n    def save_image(self, image, path):\r\n        # Save image to disk\r\n        cv2.imwrite(path, image)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"variance-generation-for-dataset-diversity",children:"Variance Generation for Dataset Diversity"}),"\n",(0,t.jsx)(n.p,{children:"To create more robust AI models, it's important to vary environmental conditions in the synthetic data:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'import random\r\n\r\nclass EnvironmentVariation:\r\n    def __init__(self, world):\r\n        self.world = world\r\n        self.stage = world.stage\r\n        \r\n    def randomize_environment(self):\r\n        # Randomize lighting\r\n        self.randomize_lighting()\r\n        \r\n        # Randomize object positions\r\n        self.randomize_object_positions()\r\n        \r\n        # Randomize textures and materials\r\n        self.randomize_materials()\r\n        \r\n        # Randomize weather conditions (if supported)\r\n        self.randomize_weather()\r\n    \r\n    def randomize_lighting(self):\r\n        # Find the dome light and randomize its properties\r\n        dome_light = self.stage.GetPrimAtPath("/World/DomeLight")\r\n        if dome_light.IsValid():\r\n            # Randomize intensity\r\n            intensity = random.uniform(500, 1500)\r\n            UsdLux.DomeLight(dome_light).CreateIntensityAttr(intensity)\r\n            \r\n            # Randomize color temperature\r\n            color = Gf.Vec3f(random.uniform(0.8, 1.2), \r\n                            random.uniform(0.8, 1.2), \r\n                            random.uniform(0.8, 1.2))\r\n            UsdLux.DomeLight(dome_light).CreateColorAttr(color)\r\n    \r\n    def randomize_object_positions(self):\r\n        # Get all objects in the scene\r\n        objects = [prim for prim in self.stage.Traverse() \r\n                   if prim.GetTypeName() in ["Xform", "Mesh"] and \r\n                   "World/Object" in str(prim.GetPath())]\r\n        \r\n        for obj_prim in objects:\r\n            # Get current position\r\n            xform_api = UsdGeom.Xformable(obj_prim)\r\n            transform = xform_api.ComputeLocalToWorldTransform(0)\r\n            \r\n            # Apply random translation\r\n            pos = transform.ExtractTranslation()\r\n            new_pos = Gf.Vec3d(\r\n                pos[0] + random.uniform(-0.5, 0.5),\r\n                pos[1] + random.uniform(-0.5, 0.5),\r\n                pos[2] + random.uniform(0, 0.5)  # keep above ground\r\n            )\r\n            \r\n            # Set new position\r\n            xform_api.AddTranslateOp().Set(new_pos)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"isaac-ros-accelerated-vslam",children:"Isaac ROS Accelerated VSLAM"}),"\n",(0,t.jsx)(n.p,{children:"Isaac ROS includes hardware-accelerated Visual SLAM (VSLAM) packages that leverage NVIDIA GPUs for real-time performance:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import rclpy\r\nfrom rclpy.node import Node\r\nfrom sensor_msgs.msg import Image, CameraInfo\r\nfrom geometry_msgs.msg import PoseStamped\r\nfrom nav_msgs.msg import Odometry\r\nfrom std_msgs.msg import Header\r\nimport numpy as np\r\n\r\nclass IsaacVSLAMNode(Node):\r\n    def __init__(self):\r\n        super().__init__('isaac_vslam_node')\r\n        \r\n        # Subscribe to camera topics\r\n        self.image_sub = self.create_subscription(\r\n            Image,\r\n            '/camera/rgb/image_rect_color',\r\n            self.image_callback,\r\n            10\r\n        )\r\n        \r\n        self.camera_info_sub = self.create_subscription(\r\n            CameraInfo,\r\n            '/camera/rgb/camera_info',\r\n            self.camera_info_callback,\r\n            10\r\n        )\r\n        \r\n        # Publish pose estimates\r\n        self.pose_pub = self.create_publisher(PoseStamped, '/camera/pose', 10)\r\n        self.odom_pub = self.create_publisher(Odometry, '/camera/odometry', 10)\r\n        \r\n        # Initialize Isaac VSLAM backend\r\n        self.initialize_vslam_backend()\r\n        \r\n    def initialize_vslam_backend(self):\r\n        # Initialize Isaac's hardware-accelerated VSLAM\r\n        # This would connect to Isaac's VSLAM libraries\r\n        self.get_logger().info('Isaac VSLAM backend initialized')\r\n        \r\n    def image_callback(self, msg):\r\n        # Process image through Isaac's accelerated VSLAM\r\n        # The actual implementation would use Isaac's libraries\r\n        \r\n        # For demonstration, we'll mock the position estimation\r\n        estimated_position = self.process_vslam(msg)\r\n        \r\n        # Publish pose\r\n        pose_msg = self.create_pose_message(estimated_position)\r\n        self.pose_pub.publish(pose_msg)\r\n        \r\n    def process_vslam(self, image_msg):\r\n        # This would use Isaac's GPU-accelerated VSLAM\r\n        # For demo, returning a mock position\r\n        return (0.0, 0.0, 0.0)  # x, y, z position\r\n    \r\n    def create_pose_message(self, position):\r\n        pose = PoseStamped()\r\n        pose.header.stamp = self.get_clock().now().to_msg()\r\n        pose.header.frame_id = \"map\"\r\n        pose.pose.position.x = position[0]\r\n        pose.pose.position.y = position[1]\r\n        pose.pose.position.z = position[2]\r\n        return pose\n"})}),"\n",(0,t.jsx)(n.h2,{id:"applications-of-synthetic-data",children:"Applications of Synthetic Data"}),"\n",(0,t.jsx)(n.p,{children:"Synthetic data from Isaac Sim can be used for:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception Model Training"}),": Training deep learning models for object detection, segmentation, etc."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sim-to-Real Transfer"}),": Pre-training models in simulation before fine-tuning with real data"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Data Augmentation"}),": Adding synthetic data to real-world datasets"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Edge Case Generation"}),": Creating rare but important scenarios that are difficult to capture in real data"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Domain Randomization"}),": Vary environmental parameters to improve model generalization"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Realistic Physics"}),": Ensure physical properties match real-world counterparts"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Sensor Accuracy"}),": Model sensor noise and limitations realistically"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Validation"}),": Compare synthetic and real data to ensure similarity"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Continue to ",(0,t.jsx)(n.a,{href:"/AI_Book_Hackathon/docs/modules/module-3-ai-brain/lesson-3-isaac-ros-nav2",children:"Lesson 3: Isaac ROS and Nav2 for Bipedal Navigation"})," to learn how Isaac ROS integrates with Nav2 for path planning and navigation."]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>o});var a=r(6540);const t={},i=a.createContext(t);function s(e){const n=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),a.createElement(i.Provider,{value:n},e.children)}}}]);